{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chatbot.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6xH0gzqkjyMx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"6d76929e-4c7e-4f7f-dacc-38c3f37450d1","executionInfo":{"status":"ok","timestamp":1572997341269,"user_tz":-480,"elapsed":3132,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["from __future__ import absolute_import, division, print_function\n","# Import TensorFlow >= 1.10 and enable eager execution\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","tf.enable_eager_execution()\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VhnXPqhFj9bv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":120},"outputId":"fc73b368-bfcb-4232-dc75-610d50158c81","executionInfo":{"status":"ok","timestamp":1572997373338,"user_tz":-480,"elapsed":35190,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QhbgUzdbkAzD","colab_type":"code","colab":{}},"source":["#path to settings file\n","#get input from user\n","dataset_folder = '/content/gdrive/My Drive/ChatBot_Dataset/twitter_30K/'\n","settings_file_path = dataset_folder + 'settings_qam3c.json'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IzsuYKJnkDEo","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"b92432ff-39f6-45ce-8673-eb5f6517d2cc","executionInfo":{"status":"ok","timestamp":1572997378014,"user_tz":-480,"elapsed":39857,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","import json\n","import unicodedata\n","import re\n","from datetime import datetime\n","from __future__ import absolute_import, division, print_function\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import os\n","import time\n","\n","\n","class Settings():\n","    def __init__ (self,settings_file_path,parent_folder):\n","        self.settings = settings_file_path\n","        self.read_settings(parent_folder )\n","\n","    def read_settings(self,parent_folder):\n","        with open(self.settings) as f:\n","            jsonsettings = json.load(f)\n","            \n","        self.model_path = parent_folder + jsonsettings[\"model_path\"]\n","        self.qam3_path = parent_folder + jsonsettings[\"qam3_path\"]\n","        self.qam5_path = parent_folder + jsonsettings[\"qam5_path\"]\n","        self.coreref_path = parent_folder + jsonsettings[\"coreref_path\"]\n","       \n","        self.training_set_path = parent_folder + jsonsettings[\"training_set_path\"]\n","        self.validation_set_path = parent_folder + jsonsettings[\"validation_set_path\"]\n","        self.test_set_path = parent_folder + jsonsettings[\"test_set_path\"]\n","        self.training_size = jsonsettings[\"training_size\"]\n","        self.validation_size = jsonsettings[\"validation_size\"]\n","        \n","        self.embedding_type = jsonsettings[\"embedding_type\"]\n","        self.encoder_type = jsonsettings[\"encoder_type\"]\n","        self.rnn_type = jsonsettings[\"rnn_type\"]\n","        self.internal_feedback = jsonsettings[\"internal_feedback\"]\n","        self.dropout = jsonsettings[\"dropout\"]\n","        self.recurrent_dropout = jsonsettings[\"recurrent_dropout\"]\n","        \n","        self.checkpoint_path = self.model_path + '/' + jsonsettings[\"checkpoint_path\"]\n","        self.checkpoint_path_qam3 = self.qam3_path + '/' + jsonsettings[\"checkpoint_path\"]\n","        self.checkpoint_path_qam5 = self.qam5_path + '/' + jsonsettings[\"checkpoint_path\"]\n","        self.checkpoint_path_coreref = self.coreref_path + '/' + jsonsettings[\"checkpoint_path\"]\n","\n","        self.embedding_dim = jsonsettings[\"embedding_dim\"]\n","        self.units = jsonsettings[\"units\"]\n","        self.epochs = jsonsettings[\"epochs\"]\n","        self.batch_size = jsonsettings[\"batch_size\"]\n","        self.beam_size = jsonsettings[\"beam_size\"]\n","\n","        self.input_vocab_path = parent_folder  + jsonsettings[\"input_vocab_path\"]\n","        self.input_vocab_size = jsonsettings[\"input_vocab_size\"]\n","        self.output_vocab_path = parent_folder  + jsonsettings[\"output_vocab_path\"]\n","        self.output_vocab_size = jsonsettings[\"output_vocab_size\"]\n","        self.label_vocab_path_qam3 = parent_folder + jsonsettings[\"label_vocab_path_qam3\"]\n","        self.label_vocab_size_qam3 = jsonsettings[\"label_vocab_size_qam3\"]\n","        self.label_vocab_path_qam5 = parent_folder + jsonsettings[\"label_vocab_path_qam5\"]\n","        self.label_vocab_size_qam5 = jsonsettings[\"label_vocab_size_qam5\"]\n","        self.label_vocab_path_coreref = parent_folder + jsonsettings[\"label_vocab_path_coreref\"]\n","        self.label_vocab_size_coreref = jsonsettings[\"label_vocab_size_coreref\"]\n","\n","        self.input_max_len = jsonsettings[\"input_max_len\"]\n","        self.output_max_len = jsonsettings[\"output_max_len\"]\n","        self.label_max_len = jsonsettings[\"label_max_len\"]\n","        \n","        self.training_loss_path = self.model_path + '/' + jsonsettings[\"training_loss_path\"]\n","        self.validation_loss_path = self.model_path + '/' + jsonsettings[\"validation_loss_path\"]\n","        self.predictions_path = self.model_path + '/' + jsonsettings[\"predictions_path\"]\n","\n","    def update_settings(self,jsonfilepath,key,value):\n","        jsonFile = open(jsonfilepath, \"r\") # Open the JSON file for reading\n","        data = json.load(jsonFile) # Read the JSON into the buffer\n","        jsonFile.close() # Close the JSON file\n","    \n","        data[key] = str(value)\n","    \n","        ## Save our changes to JSON file\n","        jsonFile = open(jsonfilepath, \"w\")\n","        jsonFile.write(json.dumps(data))\n","        jsonFile.close()\n","                \n","# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","def removelastquote(word):\n","    if len(word) > 1:\n","        if (( word[0]) == \"'\"):\n","           word = word [1:]\n","        if (( word[len(word)-1]) == \"'\"):\n","           word = word [0:-1]\n","        return word\n","    else:\n","        return word\n","      \n","def clean_string(line):\n","    line = line.lower()\n","    #line = change_quote(line)\n","    #line = line.replace(\"'\",\"\")\n","    #line = line.replace(\"`\",\"'\")\n","    line = re.sub(r\"[^a-z0123456789'!?,.-]+\", \" \", line)\n","    line = re.sub(r'[\"?\"]+', \" ? \", line)\n","    line = re.sub(r'[\".\"]+', \" . \", line)\n","    line = re.sub(r'[\",\"]+', \" , \", line)\n","    line = re.sub(r'[\"!\"]+', \" ! \", line)\n","    line = re.sub(r'[\" \"]+', \" \", line)\n","    return line.strip()\n","\n","def preprocess_sentence_word(sentence):\n","    sentence = clean_string(sentence)  \n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    sentence = '<start> ' + sentence + ' <end>'\n","    return sentence\n","\n","def preprocess_user_input_word(sentence,input_vocab):\n","    sentence = clean_string(sentence)  \n","    #to do: may be try adding a <unk> token instead\n","    new_sentence=\"\"\n","    for word in sentence.split(' '):\n","        if word in input_vocab.idx2word.values():\n","            new_sentence = new_sentence + word + ' '\n","        else:\n","            new_sentence = new_sentence + '<unk>' + ' '\n","    #new_sentence = re.sub(r'[\" \"]+', \" \", new_sentence)\n","    new_sentence = new_sentence.strip()\n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    new_sentence = '<start> ' + new_sentence + ' <end>'\n","    #print(new_sentence)\n","    return new_sentence\n","\n","def reverse_sentence(sentence):\n","    wordlist=\"\"\n","    wordlist=sentence.split(\" \")\n","    reversed_wordlist=wordlist[-1::-1]\n","    reversed_str= ' '.join(reversed_wordlist)\n","    return reversed_str\n","\n","def get_dataset_size(path):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    return len(lines)\n","\n","def create_dataset_word(path, num_examples):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    word_pairs = [[preprocess_sentence_word(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","    return word_pairs\n","\n","def create_dataset_word_qam3classes(path, num_examples):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    #word_pairs = [[preprocess_sentence_word(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","    dataset=[]\n","    for qaline in lines[:num_examples]:\n","        #print(qaline)\n","        quest,answr1,match1,answr2,match2,answr3,match3= qaline.split('\\t')\n","        quest_processed= preprocess_sentence_word(quest)\n","        answr1_processed= preprocess_sentence_word(answr1)\n","        answr2_processed= preprocess_sentence_word(answr2)\n","        answr3_processed= preprocess_sentence_word(answr3)\n","        dataset.append([quest_processed,answr1_processed,match1,answr2_processed,match2,answr3_processed,match3])\n","    return dataset\n","\n","def create_dataset_word_qam(path, num_examples):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    #word_pairs = [[preprocess_sentence_word(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","    dataset=[]\n","    for qaline in lines[:num_examples]:\n","        #print(qaline)\n","        quest,answr,match= qaline.split('\\t')\n","        quest_processed= preprocess_sentence_word(quest)\n","        answr_processed= preprocess_sentence_word(answr)\n","        dataset.append([quest_processed,answr_processed,match])\n","    return dataset\n","\n","\n","class EmbeddingIndex():\n","  def __init__(self, sentence):\n","    self.sentence = sentence\n","    self.word2idx = {}\n","    self.idx2word = {}\n","    self.vocab = set()\n","    \n","    self.create_index()\n","    \n","  def create_index(self):\n","    for phrase in self.sentence:\n","      self.vocab.update(phrase.split(' '))\n","    \n","    self.vocab = sorted(self.vocab)\n","    \n","    self.word2idx['<pad>'] = 0\n"," \n","    for index, word in enumerate(self.vocab):\n","        self.word2idx[word] = index + 1\n","    \n","    for word, index in self.word2idx.items():\n","      self.idx2word[index] = word\n","\n","      \n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def remove_last_empty_line(path):\n","    with open(path) as f_input:\n","        data = f_input.read().rstrip('\\n')\n","    with open(path, 'w') as f_output:    \n","        f_output.write(data)\n","        \n","def remove_empty_lines(path):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    with open(path, 'w') as f:   \n","        for line in lines:\n","            if not line.strip(): \n","                continue  # skip the empty line\n","            else:\n","                 f.write(\"{}\\n\".format(line))\n","        \n","def get_vocab(path):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    vocab = EmbeddingIndex(word for word in lines)\n","    return vocab\n","\n","def load_dataset_word(path, num_examples, input_vocab, output_vocab):\n","    # creating cleaned input, output pairs\n","    pairs = create_dataset_word(path, num_examples)\n","\n","    # Input sentences\n","    input_tensor = [[input_vocab.word2idx[word] for word in input_sentence.split(' ')] for input_sentence, output_sentence in pairs]\n","\n","    # Reverse Input sentences (as suggested in sutskever)\n","    #comment this out for now\n","    #reversed_input_tensor = [[input_vocab.word2idx[word] for word in reverse_sentence(input_sentence).split(' ')] for input_sentence, output_sentence in pairs]\n","    \n","    # Output sentences\n","    target_tensor = [[output_vocab.word2idx[word] for word in output_sentence.split(' ')] for input_sentence, output_sentence in pairs]\n","    \n","    # Calculate max_length of input and output tensor\n","    # Here, we'll set those to the longest sentence in the dataset\n","    #max_length_inp, max_length_reversed_input, max_length_tar = max_length(input_tensor), max_length(reversed_input_tensor), max_length(target_tensor)\n","    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n","    \n","    # Padding the input and output tensor to the maximum length\n","    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp,  padding='post')\n","    \n","    #reversed_input_tensor = tf.keras.preprocessing.sequence.pad_sequences(reversed_input_tensor,  maxlen=max_length_reversed_input, padding='post')\n","    \n","    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_tar,  padding='post')\n","    \n","    #return input_tensor, reversed_input_tensor, target_tensor, max_length_inp, max_length_reversed_input, max_length_tar     \n","    return input_tensor, target_tensor, max_length_inp, max_length_tar     \n","\n","def load_dataset_word_qam3classes(path, num_examples, input_vocab, output_vocab, label_vocab):\n","    # creating cleaned input, output pairs\n","    dataset = create_dataset_word_qam3classes(path, num_examples)\n","\n","    # Input sentences\n","    input_tensor = [[input_vocab.word2idx[word] for word in input_sentence.split(' ')] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","    \n","    # Correct answer\n","    output1_tensor = [[output_vocab.word2idx[word] for word in output1.split(' ')] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","    output1_label_tensor = [[label_vocab.word2idx[label1]] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","\n","    # partially correct answer\n","    output2_tensor = [[output_vocab.word2idx[word] for word in output2.split(' ')] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","    output2_label_tensor = [[label_vocab.word2idx[label2]] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","\n","    # wrong answer\n","    output3_tensor = [[output_vocab.word2idx[word] for word in output3.split(' ')] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","    output3_label_tensor = [[label_vocab.word2idx[label3]] for input_sentence, output1, label1 , output2, label2, output3, label3 in dataset]\n","\n","    # Calculate max_length of input and output tensor\n","    # Here, we'll set those to the longest sentence in the dataset\n","    #max_length_inp, max_length_reversed_input, max_length_tar = max_length(input_tensor), max_length(reversed_input_tensor), max_length(target_tensor)\n","    max_length_inp, max_length_tar, max_length_label = max_length(input_tensor), max_length(output1_tensor), max_length(output1_label_tensor)\n","    \n","    # Padding the input and output tensor to the maximum length\n","    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp,  padding='post')\n","    \n","    output1_tensor = tf.keras.preprocessing.sequence.pad_sequences(output1_tensor, maxlen=max_length_tar,  padding='post')\n","    output2_tensor = tf.keras.preprocessing.sequence.pad_sequences(output2_tensor, maxlen=max_length_tar,  padding='post')\n","    output3_tensor = tf.keras.preprocessing.sequence.pad_sequences(output3_tensor, maxlen=max_length_tar,  padding='post')\n","    \n","    output1_label_tensor = tf.keras.preprocessing.sequence.pad_sequences(output1_label_tensor, maxlen=max_length_label,  padding='post')\n","    output2_label_tensor = tf.keras.preprocessing.sequence.pad_sequences(output2_label_tensor, maxlen=max_length_label,  padding='post')\n","    output3_label_tensor = tf.keras.preprocessing.sequence.pad_sequences(output3_label_tensor, maxlen=max_length_label,  padding='post')\n","    \n","    #return input_tensor, reversed_input_tensor, target_tensor, max_length_inp, max_length_reversed_input, max_length_tar     \n","    return input_tensor, output1_tensor, output1_label_tensor, output2_tensor, output2_label_tensor, output3_tensor, output3_label_tensor\n","\n","def log_epoch_loss(path,epoch, loss):\n","    now=datetime.today().isoformat()\n","    with open(path, 'a') as log_file:\n","        log_file.write(\"{},{},{}\\n\".format(now,epoch,loss))\n","        \n","#MODEL\n","def gru(units,dropout, recurrent_dropout,go_backwards=False):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","  if tf.test.is_gpu_available():\n","    return tf.keras.layers.CuDNNGRU(units, \n","                                    return_sequences=True, \n","                                    return_state=True, \n","                                    recurrent_initializer='glorot_uniform',\n","                                    go_backwards=go_backwards)\n","  else:\n","    return tf.keras.layers.GRU(units, \n","                               return_sequences=True, \n","                               return_state=True, \n","                               dropout=dropout,\n","                               recurrent_dropout=recurrent_dropout,\n","                               recurrent_activation='sigmoid', \n","                               recurrent_initializer='glorot_uniform',\n","                               go_backwards=go_backwards)\n","    \n","def lstm(units,dropout, recurrent_dropout,go_backwards=False):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","  #dropout not supported in GPU\n","  if tf.test.is_gpu_available():\n","    return tf.keras.layers.CuDNNLSTM(units, \n","                                    return_sequences=True, \n","                                    return_state=True, \n","                                    recurrent_initializer='glorot_uniform',\n","                                    go_backwards=go_backwards)\n","  else:\n","    return tf.keras.layers.LSTM(units, \n","                               return_sequences=True, \n","                               return_state=True,\n","                               dropout=dropout,\n","                               recurrent_dropout=recurrent_dropout,\n","                               recurrent_activation='sigmoid', \n","                               recurrent_initializer='glorot_uniform',\n","                               go_backwards=go_backwards)\n","  \n","class Encoder_BidirectionalGRU(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout,recurrent_dropout):\n","        super(Encoder_BidirectionalGRU, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru_fw = gru(self.enc_units,dropout,recurrent_dropout)\n","        self.gru_bw = gru(self.enc_units,dropout,recurrent_dropout,True)\n","        \n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output_fw, state_fw = self.gru_fw(x, initial_state = hidden)        \n","        output_bw, state_bw = self.gru_bw(x, initial_state = hidden)  \n","        output =  tf.concat([output_fw,output_bw], axis=2)\n","        state =  tf.concat([state_fw,state_bw], axis=1)\n","        return output, state, x\n","    \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","      \n","\n","class Encoder_BidirectionalLSTM(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout,recurrent_dropout):\n","        super(Encoder_BidirectionalLSTM, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm_fw = lstm(self.enc_units,dropout,recurrent_dropout)\n","        self.lstm_bw = lstm(self.enc_units,dropout,recurrent_dropout,True)\n","        \n","    def call(self, x, hidden, cell_state):\n","        x = self.embedding(x)\n","        output_fw, state_fw, cell_state_fw = self.lstm_fw(x, initial_state = [hidden, cell_state])        \n","        output_bw, state_bw, cell_state_bw = self.lstm_bw(x, initial_state = [hidden, cell_state])        \n","        output =  tf.concat([output_fw,output_bw], axis=2)\n","        state =  tf.concat([state_fw,state_bw], axis=1)\n","        state =  tf.concat([cell_state_fw,cell_state_bw], axis=1)\n","        return output, state, cell_state\n","   \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","\n","        \n","class Decoder_GRU(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout,recurrent_dropout):\n","        super(Decoder_GRU, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.dec_units,dropout,recurrent_dropout)\n","           \n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","               \n","        # used for attention (Bahdanau style)\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, enc_output, hidden):\n","\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, hidden_size)\n","        #Bahdanau's additive style (Bahdanau attention)        \n","\n","        #original was tanh\n","        #score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        score = tf.nn.relu6(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","\n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","            \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","            \n","        #return x, state, attention_weights\n","        return x, state\n"," \n","       \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))\n","         \n","class Decoder_LSTM(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout,recurrent_dropout):\n","        super(Decoder_LSTM, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = lstm(self.dec_units,dropout,recurrent_dropout)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","               \n","        # used for attention (Bahdanau style)\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, enc_output, hidden, cell_state):\n","\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, hidden_size)\n","        #Bahdanau's additive style (Bahdanau attention)        \n","\n","        #original was tanh\n","        #score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        score = tf.nn.relu6(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the LSTM\n","        output, state, cell_state = self.lstm(x)\n","\n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","            \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","            \n","        #return x, state, attention_weights\n","        return x, state, cell_state\n","           \n","       \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))\n","\n","class SimilarityDecoder(tf.keras.Model):\n","    def __init__(self, vocab_size, units, dropout):\n","        super(SimilarityDecoder, self).__init__()\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, context_vector_question, context_vector_answer):\n","\n","        context_vector = context_vector_question * context_vector_answer\n","        \n","        x=self.fc(context_vector)\n","       \n","        return x\n","        \n","class SimilarityDecoder_RYANLOWE(tf.keras.Model):\n","    def __init__(self, vocab_size, units, dropout):\n","        super(SimilarityDecoder_RYANLOWE, self).__init__()\n","        \n","        self.M = tf.keras.layers.Dense(units*2)\n","        #self.fc = tf.keras.layers.Dense(vocab_size)\n","        self.fc = tf.keras.layers.Dense(vocab_size,activation='softmax')\n","        \n","    def call(self, context_vector_question, context_vector_answer):\n","\n","        predicted_question = self.M(context_vector_answer)\n","        dot_product = predicted_question * context_vector_question\n","        x=self.fc(dot_product)\n","        return x\n","      \n","def generate_samples(input_file, sample_file,lines_count, samples_count):\n","    import itertools\n","    import random\n","\n","    def random_gen(low, high):\n","        while True:\n","            yield random.randrange(low, high)\n","\n","    gen = random_gen(1, lines_count)\n","\n","    items = set()\n","\n","    # try to add elem to set until set length is less than 10\n","    for x in itertools.takewhile(lambda x: len(items) < samples_count+1, gen): \n","        items.add(x)\n","    #print(items)\n","  \n","    with open(sample_file, 'w') as f:\n","        lines = open(input_file, encoding='UTF-8', errors='ignore').read().strip().split('\\n')\n","        line_number=0\n","        for line in lines:\n","            line_number  += 1\n","            if line_number in items:\n","                f.write(\"{}\\n\".format(line))\n","    \n","    remove_empty_lines(sample_file)\n","    remove_last_empty_line(sample_file)\n","    \n","def get_samples(path):\n","    questions=[]\n","    answers=[]\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    lines2= filter(lambda x: not x.isspace(), lines)\n","    for line in lines2:\n","        questions.append(line.split('\\t')[0])\n","        answers.append(line.split('\\t')[1])\n","    return questions, answers\n","\n","def loss_function(real, pred):\n","  mask = 1 - np.equal(real, 0)\n","  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n","  return tf.reduce_mean(loss_)\n","\n","def get_test_questions(path, start_row, num_examples):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    end_row=start_row+num_examples\n","    word_pairs = [l.split('\\t') for l in lines[start_row:end_row]]\n","    return word_pairs\n","\n","def greedy_search_prediction(question, rnntype, encoder, decoder, input_vocab, output_vocab, input_max_len, output_max_len):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result = ''\n","\n","    hidden = encoder.initialize_states()\n","    if rnntype == 'LSTM':\n","        enc_cell = encoder.initialize_states()\n","        dec_cell = decoder.initialize_states()\n","        enc_out, enc_hidden, enc_cell = encoder(inputs, hidden, enc_cell)\n","    else:\n","        enc_out, enc_hidden = encoder(inputs, hidden)\n","        \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","\n","    #print(output_vocab.idx2word)\n","    score_highest=[]\n","    #score_lowest=[]\n","    #score_mean=[]\n","    #score_total=[]\n","    \n","    for t in range(output_max_len):\n","      \n","        if rnntype == 'LSTM':\n","          #predictions, dec_hidden, dec_cell, attention_weights = decoder(dec_input, enc_out, dec_hidden, dec_cell )\n","          predictions, dec_hidden, dec_cell = decoder(dec_input, enc_out, dec_hidden, dec_cell )\n","        else:\n","          #predictions, dec_hidden, attention_weights = decoder(dec_input, enc_out, dec_hidden)\n","          predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","        #print(\"Predictions is: {}\".format(predictions))\n","    \n","        #predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        \n","        highest = (tf.reduce_max(predictions[0])).numpy()\n","        #lowest  = (tf.reduce_min(predictions[0])).numpy()\n","        #mean    = (tf.reduce_mean(predictions[0])).numpy()\n","        #total   = (tf.reduce_sum(predictions[0])).numpy()\n","      \n","        score_highest.append(highest)\n","        #score_lowest.append(lowest)        \n","        #score_mean.append(mean)        \n","        #score_total.append(total) \n","        \n","        result += output_vocab.idx2word[predicted_id] + ' '\n","        \n","        if output_vocab.idx2word[predicted_id] == '<end>':\n","            #return result, sentence, attention_plot\n","            high_score = sum(score_highest)/(t+1)\n","            #low_score = sum(score_lowest)/(t+1)    \n","            #mean_score = sum(score_mean)/(t+1)\n","            #total_score = sum(score_total)/(t+1)    \n","            #bleu_score = calculate_bleu(clean_string(reference_answer),clean_string(result))\n","            return result, high_score\n","        \n","            # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","    \n","    #return result, sentence, attention_plot\n","    high_score = sum(score_highest)/(t+1)\n","    #low_score = sum(score_lowest)/(t+1)    \n","    #mean_score = sum(score_mean)/(t+1)    \n","    #total_score = sum(score_total)/(t+1)   \n","    #bleu_score = calculate_bleu(reference_answer,result)    \n","    \n","    return result, high_score\n","\n","def get_k_top_predictions(predictions,avg_sent_prob ,predicted_sentence, dec_input,k=1):\n","    #the reason sentence_prob,predicted_sentence, dec_input is passed is that i can use it back unchanged\n","    #there is no processing done for these values bcos i need it unchnaged in the calling function\n","    k_top_predictions = tf.nn.top_k(predictions[0],k)\n","    predictedids = k_top_predictions[1].numpy()\n","    probabilities = k_top_predictions[0].numpy()\n","    #create is, prob pair of list\n","    id_prob_pair=[]\n","    for idx in range(k):\n","        id_prob_pair.append([predictedids[idx],probabilities[idx],avg_sent_prob ,predicted_sentence, dec_input])\n","    return id_prob_pair\n","    \n","\n","def beam_search_prediction(question, rnntype, encoder, decoder, input_vocab, output_vocab, input_max_len, output_max_len,k=1):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    if rnntype == 'LSTM':\n","        enc_cell = encoder.initialize_states()\n","        dec_cell = decoder.initialize_states()\n","        enc_out, enc_hidden, enc_cell = encoder(inputs, hidden, enc_cell)\n","    else:\n","        enc_out, enc_hidden , embedding = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    #idea from https://github.com/mmehdig/lm_beam_search/blob/master/beam_search.py\n","    #K_beam[prob,result,dec_input,dec_hidden]\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    \n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","\n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    #predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","                    predicted_sentence += '<end>' \n","                    total_sent_prob += word_prob\n","                    dec_input = tf.expand_dims([predicted_id], 0)\n","                    all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","                    total_sent_prob += word_prob\n","                    dec_input = tf.expand_dims([predicted_id], 0)\n","                    all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","            #k_beam = sorted(all_k_beams)[:k]\n","            predicted_sent_tmp = k_beam[0][1]\n","            if '<end>' in predicted_sent_tmp:\n","                avg_sent_prob = (k_beam[0][0])/(t+1)\n","                return predicted_sent_tmp, avg_sent_prob\n","\n","    #get top 1 for now\n","    t_beam = sorted(k_beam,reverse=True)[:1]\n","    #t_beam = sorted(k_beam)[:1]\n","    #sentence_prob_pair=[]\n","    #for sentence_prob, total_sent_prob, predicted_sentence, _ , _ in t_beam:\n","    #    avg_sent_prob = total_sent_prob/t\n","    #    sentence_prob_pair.append([sentence_prob, avg_sent_prob, predicted_sentence])\n","\n","    predicted_sent = t_beam[0][1]\n","    #if t==0:\n","    #    avg_sent_prob = 0\n","    #else:\n","    #need to get average\n","    avg_sent_prob = (t_beam[0][0])/(t+1)\n","    return predicted_sent, avg_sent_prob\n","\n","def remove_everything_after_end_token(sentence):\n","    searchStr = '<end>'\n","    return sentence[:sentence.find(searchStr)]\n","\n","print(\"Helper Functions Done\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Helper Functions Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"exBvxyz9kFiu","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"302d87b4-020d-47b4-a0db-d89619da8c36","executionInfo":{"status":"ok","timestamp":1572997380065,"user_tz":-480,"elapsed":41901,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","#Variables' definitions\n","\n","param = Settings(settings_file_path,dataset_folder)\n","corpus_path = param.training_set_path#input from user\n","param = Settings(settings_file_path,dataset_folder)\n","training_set_path = param.training_set_path\n","validation_set_path = param.validation_set_path\n","input_vocab_path=param.input_vocab_path\n","output_vocab_path=param.output_vocab_path\n","label_vocab_path=param.label_vocab_path_qam3\n","checkpoint_path = param.checkpoint_path\n","epochs = int(param.epochs)\n","batch_size = int(param.batch_size)\n","embedding_dim = int(param.embedding_dim)\n","units  = int(param.units)\n","training_loss_path = param.training_loss_path\n","validation_loss_path = param.validation_loss_path\n","encoder_type=param.encoder_type\n","embedding_type=param.embedding_type\n","rnn_type=param.rnn_type\n","training_size = int(param.training_size)\n","validation_size = int(param.validation_size)\n","dropout=float(param.dropout)\n","recurrent_dropout=float(param.recurrent_dropout)\n","internal_feedback=param.internal_feedback\n","test_questions_path=param.test_set_path\n","predictions_path=param.predictions_path\n","input_max_len = int(param.input_max_len)\n","output_max_len = int(param.output_max_len)\n","beam_size=int(param.beam_size)\n","rnntype=rnn_type.upper()\n","input_vocab = get_vocab(input_vocab_path)\n","output_vocab = get_vocab(output_vocab_path)\n","label_vocab = get_vocab(label_vocab_path)\n","input_vocab_size = len(input_vocab.word2idx)\n","output_vocab_size = len(output_vocab.word2idx)\n","label_vocab_size=len(label_vocab.word2idx)\n","\n","rnntype=rnn_type.upper()\n","\n","print('Model Details')\n","print('Training size is {}'.format(training_size))\n","print(\"Vocab size is {}:{}\".format(input_vocab_size,output_vocab_size))\n","print('Epoch count is {}'.format(epochs))\n","print('Embedding size is {}'.format(embedding_dim))\n","print('Hidden units is {}'.format(units))\n","print('Batch size is {}'.format(batch_size))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model Details\n","Training size is 30000\n","Vocab size is 12270:11471\n","Epoch count is 20\n","Embedding size is 512\n","Hidden units is 512\n","Batch size is 32\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHkufZp8kPuR","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"d7a466b7-1682-4aca-a88d-97bd7f6a135a","executionInfo":{"status":"ok","timestamp":1572997380066,"user_tz":-480,"elapsed":41898,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","\"\"\"\n","#Testing - standard beam search\n","batch_size=1   \n","if rnntype == 'GRU':\n","    qam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","    qam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","    qam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","    qam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\n","    \n","optimizer = tf.train.AdamOptimizer() #tf v1\n","#optimizer = tf.optimizers.Adam()  #tf v2\n","\n","#prepare checkpoint details\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Checkpoint file is missing\")\n","\n","start_row=0\n","test_size=1000\n","QA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\n","print('Questions path {}'.format(test_questions_path))\n","qa_pair=[]\n","\n","filename, file_extension = os.path.splitext(predictions_path)\n","#beam_search_filename = filename + 'BS_' +  embedding_type + '_'+ encoder_type +'_'+ rnn_type +'_'+ str(dropout) +'_'+ internal_feedback + '.txt'\n","beam_search_filename = filename + '_standard_bs_QAM3.txt'\n","print('File path: {}'.format(beam_search_filename))\n","#predictions_path_gs = filename + '_greedy.txt'\n","#predictions_path_bs = filename + '_bs_' + str(beam_size) + '.csv'\n","#just focus on beam search  \n","\n","qa_pair_bs=[]\n","\n","#beam search\n","for question, answer in QA_Pairs:\n","    if embedding_type == 'word':\n","        #takes care for char and mix\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        cleaned_input = preprocess_sentence_char(question)\n","    \n","    if len(cleaned_input.strip()):\n","        reply,  score = beam_search_prediction(cleaned_input, rnntype, qam3_question_encoder, qam3_answer_decoder, input_vocab, output_vocab, input_max_len, output_max_len,beam_size)    \n","        \n","        if embedding_type == 'char':\n","            reply = preprocess_output_char(reply)\n"," \n","        reply = re.sub('<start>', '', reply)\n","        #reply = re.sub('<end>', '', reply)\n","        \n","        idx = reply.find('<end>')\n","        if idx > 0:\n","            reply = reply[:idx]\n","        else:\n","            reply = re.sub('<end>', '', reply)\n","        \n","        reply = re.sub('<pad>', '', reply)\n","        #print(reply)\n","        qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","    else:\n","        reply = 'Zero words/characters in vocabulary'\n","        score=-1\n","        qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        \n","with open(beam_search_filename, 'w') as f:\n","    for line in qa_pair_bs:\n","        f.write(\"{}\\n\".format(line))\n","\n","print('Standard beam search done. File created at {}'.format(beam_search_filename))\n","\"\"\""],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#Testing - standard beam search\\nbatch_size=1   \\nif rnntype == \\'GRU\\':\\n    qam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\n    qam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\n    qam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\n    qam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\\n    \\noptimizer = tf.train.AdamOptimizer() #tf v1\\n#optimizer = tf.optimizers.Adam()  #tf v2\\n\\n#prepare checkpoint details\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\n#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\\nif os.path.exists(checkpoint_filepath):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\\n  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n  print(\"Existing checkpoint file loaded\")\\nelse:\\n  print(\"Checkpoint file is missing\")\\n\\nstart_row=0\\ntest_size=1000\\nQA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\\nprint(\\'Questions path {}\\'.format(test_questions_path))\\nqa_pair=[]\\n\\nfilename, file_extension = os.path.splitext(predictions_path)\\n#beam_search_filename = filename + \\'BS_\\' +  embedding_type + \\'_\\'+ encoder_type +\\'_\\'+ rnn_type +\\'_\\'+ str(dropout) +\\'_\\'+ internal_feedback + \\'.txt\\'\\nbeam_search_filename = filename + \\'_standard_bs_QAM3.txt\\'\\nprint(\\'File path: {}\\'.format(beam_search_filename))\\n#predictions_path_gs = filename + \\'_greedy.txt\\'\\n#predictions_path_bs = filename + \\'_bs_\\' + str(beam_size) + \\'.csv\\'\\n#just focus on beam search  \\n\\nqa_pair_bs=[]\\n\\n#beam search\\nfor question, answer in QA_Pairs:\\n    if embedding_type == \\'word\\':\\n        #takes care for char and mix\\n        cleaned_input = preprocess_user_input_word(question,input_vocab)\\n    else:\\n        cleaned_input = preprocess_sentence_char(question)\\n    \\n    if len(cleaned_input.strip()):\\n        reply,  score = beam_search_prediction(cleaned_input, rnntype, qam3_question_encoder, qam3_answer_decoder, input_vocab, output_vocab, input_max_len, output_max_len,beam_size)    \\n        \\n        if embedding_type == \\'char\\':\\n            reply = preprocess_output_char(reply)\\n \\n        reply = re.sub(\\'<start>\\', \\'\\', reply)\\n        #reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        idx = reply.find(\\'<end>\\')\\n        if idx > 0:\\n            reply = reply[:idx]\\n        else:\\n            reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        reply = re.sub(\\'<pad>\\', \\'\\', reply)\\n        #print(reply)\\n        qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n    else:\\n        reply = \\'Zero words/characters in vocabulary\\'\\n        score=-1\\n        qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n        \\nwith open(beam_search_filename, \\'w\\') as f:\\n    for line in qa_pair_bs:\\n        f.write(\"{}\\n\".format(line))\\n\\nprint(\\'Standard beam search done. File created at {}\\'.format(beam_search_filename))\\n'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"sMxCzzBBkWm3","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"b63eed3a-0c05-4073-8f83-e12cf1d4f4ae","executionInfo":{"status":"ok","timestamp":1572997380072,"user_tz":-480,"elapsed":41900,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#Testing\n","\"\"\"\n","def QAM3_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    enc_out, enc_hidden, _ = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    output_beam =[]\n","    \n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        \n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","            \n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    predicted_sentence += '<end>' \n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","\n","                total_sent_prob += word_prob\n","                dec_input = tf.expand_dims([predicted_id], 0)\n","                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","    \n","    for i in range(len(k_beam)):\n","        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\n","    \n","    updated_output_beam=[]\n","    for replies in output_beam:\n","        score = replies[0]\n","        reply = preprocess_user_input_word(replies[1] ,output_vocab)\n","        question_inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding='post')\n","        question_inputs = tf.convert_to_tensor(question_inputs )\n","\n","        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(' ')]\n","        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding='post')\n","        reply_outputs = tf.convert_to_tensor(reply_outputs)\n","\n","        question_hidden = question_encoder.initialize_states()\n","        answer_hidden = answer_encoder.initialize_states()\n","\n","        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\n","        answer_enc_output, answer_enc_hidden , _ = answer_encoder(reply_outputs, answer_hidden)\n","        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        if label_vocab.idx2word[predicted_id] != '<pad>':\n","          matching_percentage = int(label_vocab.idx2word[predicted_id])\n","        else:\n","          matching_percentage = 0\n","\n","        new_score = (score + score*matching_percentage/100)/2\n","        updated_output_beam.append([new_score,reply])\n","        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\n","    \n","    predicted_sent = sorted_updated_output_beam [0][1]\n","    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\n","    \n","    return predicted_sent, avg_sent_prob\n","print('Enhanced beam search function definition done')\n","#Testing using Enhanced Beam Search - QAM3\n","batch_size=1   \n","\n","qam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","qam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","qam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","qam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\n","    \n","optimizer = tf.train.AdamOptimizer() #tf v1\n","#optimizer = tf.optimizers.Adam()  #tf v2\n","\n","#prepare checkpoint details\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Checkpoint file is missing\")\n","\n","start_row=0\n","test_size=1000\n","QA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\n","print('Questions path {}'.format(test_questions_path))\n","qa_pair=[]\n","\n","filename, file_extension = os.path.splitext(predictions_path)\n","#beam_search_filename = filename + '_qam3_bs_QAM3_' + str(start_row) + 'To' + str(start_row+test_size) +'.txt'\n","beam_search_filename = filename + '_qam3_bs_QAM3_.txt'\n","print('File path: {}'.format(beam_search_filename))\n","\n","#qa_pair_bs=[]\n","\n","f = open(beam_search_filename, 'w')\n","\n","#beam search\n","for question, answer in QA_Pairs:\n","\n","    #print(question)\n","\n","    if embedding_type == 'word':\n","        #takes care for char and mix\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        cleaned_input = preprocess_sentence_char(question)\n","    \n","    if len(cleaned_input.strip()):\n","        \n","        reply, score = QAM3_beam_search_prediction(cleaned_input, rnntype, qam3_question_encoder, qam3_answer_decoder,  qam3_question_encoder, qam3_answer_encoder, qam3_label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,beam_size)    \n","        \n","        if embedding_type == 'char':\n","            reply = preprocess_output_char(reply)\n"," \n","        reply = re.sub('<start>', '', reply)\n","        #reply = re.sub('<end>', '', reply)\n","        \n","        idx = reply.find('<end>')\n","        if idx > 0:\n","            reply = reply[:idx]\n","        else:\n","            reply = re.sub('<end>', '', reply)\n","        \n","        reply = re.sub('<pad>', '', reply)\n","        #print(reply)\n","        #qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        line=question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() \n","        f.write(\"{}\\n\".format(line))\n","    else:\n","        reply = 'Zero words/characters in vocabulary'\n","        score=-1\n","        #qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        line=question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() \n","        f.write(\"{}\\n\".format(line))\n","        \n","#with open(beam_search_filename, 'w') as f:\n","#    for line in qa_pair_bs:\n","#        f.write(\"{}\\n\".format(line))\n","f.close()\n","print('QAM3 Beam search done. File created at {}'.format(beam_search_filename))\n","\"\"\""],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef QAM3_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\\n    #attention_plot = np.zeros((max_length_targ, max_length_inp))\\n   \\n    inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding=\\'post\\')\\n    inputs = tf.convert_to_tensor(inputs)\\n    \\n    predicted_sentence = \\'\\'\\n   \\n    hidden = encoder.initialize_states()\\n    enc_out, enc_hidden, _ = encoder(inputs, hidden)\\n    \\n    dec_hidden = enc_hidden\\n    dec_input = tf.expand_dims([output_vocab.word2idx[\\'<start>\\']], 0)\\n    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\\n    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\\n    \\n    output_beam =[]\\n    \\n    for t in range(output_max_len):\\n        all_k_beams = []\\n        \\n        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\\n            \\n            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\\n            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\\n            \\n            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\\n                    \\n                if output_vocab.idx2word[predicted_id] == \\'<end>\\':\\n                    predicted_sentence += \\'<end>\\' \\n                else:\\n                    predicted_sentence += output_vocab.idx2word[predicted_id] + \\' \\'\\n\\n                total_sent_prob += word_prob\\n                dec_input = tf.expand_dims([predicted_id], 0)\\n                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\\n                \\n            k_beam = sorted(all_k_beams,reverse=True)[:k]\\n    \\n    for i in range(len(k_beam)):\\n        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\\n    \\n    updated_output_beam=[]\\n    for replies in output_beam:\\n        score = replies[0]\\n        reply = preprocess_user_input_word(replies[1] ,output_vocab)\\n        question_inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding=\\'post\\')\\n        question_inputs = tf.convert_to_tensor(question_inputs )\\n\\n        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(\\' \\')]\\n        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding=\\'post\\')\\n        reply_outputs = tf.convert_to_tensor(reply_outputs)\\n\\n        question_hidden = question_encoder.initialize_states()\\n        answer_hidden = answer_encoder.initialize_states()\\n\\n        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\\n        answer_enc_output, answer_enc_hidden , _ = answer_encoder(reply_outputs, answer_hidden)\\n        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\\n        predicted_id = tf.argmax(predictions[0]).numpy()\\n        if label_vocab.idx2word[predicted_id] != \\'<pad>\\':\\n          matching_percentage = int(label_vocab.idx2word[predicted_id])\\n        else:\\n          matching_percentage = 0\\n\\n        new_score = (score + score*matching_percentage/100)/2\\n        updated_output_beam.append([new_score,reply])\\n        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\\n    \\n    predicted_sent = sorted_updated_output_beam [0][1]\\n    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\\n    \\n    return predicted_sent, avg_sent_prob\\nprint(\\'Enhanced beam search function definition done\\')\\n#Testing using Enhanced Beam Search - QAM3\\nbatch_size=1   \\n\\nqam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\nqam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\nqam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\nqam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\\n    \\noptimizer = tf.train.AdamOptimizer() #tf v1\\n#optimizer = tf.optimizers.Adam()  #tf v2\\n\\n#prepare checkpoint details\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\ncheckpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\\n\\nif os.path.exists(checkpoint_filepath):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\\n  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n  print(\"Existing checkpoint file loaded\")\\nelse:\\n  print(\"Checkpoint file is missing\")\\n\\nstart_row=0\\ntest_size=1000\\nQA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\\nprint(\\'Questions path {}\\'.format(test_questions_path))\\nqa_pair=[]\\n\\nfilename, file_extension = os.path.splitext(predictions_path)\\n#beam_search_filename = filename + \\'_qam3_bs_QAM3_\\' + str(start_row) + \\'To\\' + str(start_row+test_size) +\\'.txt\\'\\nbeam_search_filename = filename + \\'_qam3_bs_QAM3_.txt\\'\\nprint(\\'File path: {}\\'.format(beam_search_filename))\\n\\n#qa_pair_bs=[]\\n\\nf = open(beam_search_filename, \\'w\\')\\n\\n#beam search\\nfor question, answer in QA_Pairs:\\n\\n    #print(question)\\n\\n    if embedding_type == \\'word\\':\\n        #takes care for char and mix\\n        cleaned_input = preprocess_user_input_word(question,input_vocab)\\n    else:\\n        cleaned_input = preprocess_sentence_char(question)\\n    \\n    if len(cleaned_input.strip()):\\n        \\n        reply, score = QAM3_beam_search_prediction(cleaned_input, rnntype, qam3_question_encoder, qam3_answer_decoder,  qam3_question_encoder, qam3_answer_encoder, qam3_label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,beam_size)    \\n        \\n        if embedding_type == \\'char\\':\\n            reply = preprocess_output_char(reply)\\n \\n        reply = re.sub(\\'<start>\\', \\'\\', reply)\\n        #reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        idx = reply.find(\\'<end>\\')\\n        if idx > 0:\\n            reply = reply[:idx]\\n        else:\\n            reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        reply = re.sub(\\'<pad>\\', \\'\\', reply)\\n        #print(reply)\\n        #qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n        line=question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() \\n        f.write(\"{}\\n\".format(line))\\n    else:\\n        reply = \\'Zero words/characters in vocabulary\\'\\n        score=-1\\n        #qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n        line=question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() \\n        f.write(\"{}\\n\".format(line))\\n        \\n#with open(beam_search_filename, \\'w\\') as f:\\n#    for line in qa_pair_bs:\\n#        f.write(\"{}\\n\".format(line))\\nf.close()\\nprint(\\'QAM3 Beam search done. File created at {}\\'.format(beam_search_filename))\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WSYR90juHXD1","colab":{"base_uri":"https://localhost:8080/","height":455},"outputId":"87ec6298-9db1-4dee-c9d4-9c66c1107129"},"source":["#Prediction Chatbot\n","\n","def QAM3_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    enc_out, enc_hidden, _ = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    output_beam =[]\n","    \n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        \n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","            \n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    predicted_sentence += '<end>' \n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","\n","                total_sent_prob += word_prob\n","                dec_input = tf.expand_dims([predicted_id], 0)\n","                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","    \n","    for i in range(len(k_beam)):\n","        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\n","    \n","    updated_output_beam=[]\n","    for replies in output_beam:\n","        score = replies[0]\n","        reply = preprocess_user_input_word(replies[1] ,output_vocab)\n","        question_inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding='post')\n","        question_inputs = tf.convert_to_tensor(question_inputs )\n","\n","        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(' ')]\n","        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding='post')\n","        reply_outputs = tf.convert_to_tensor(reply_outputs)\n","\n","        question_hidden = question_encoder.initialize_states()\n","        answer_hidden = answer_encoder.initialize_states()\n","\n","        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\n","        answer_enc_output, answer_enc_hidden , _ = answer_encoder(reply_outputs, answer_hidden)\n","        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        if label_vocab.idx2word[predicted_id] != '<pad>':\n","          matching_percentage = int(label_vocab.idx2word[predicted_id])\n","        else:\n","          matching_percentage = 0\n","\n","        new_score = (score + score*matching_percentage/100)/2\n","        updated_output_beam.append([new_score,reply])\n","        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\n","    \n","    predicted_sent = sorted_updated_output_beam [0][1]\n","    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\n","    \n","    return predicted_sent, avg_sent_prob\n","\n","#print('Enhanced beam search function definition done')\n","#Testing using Enhanced Beam Search - QAM3\n","batch_size=1   \n","\n","qam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","qam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","qam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","qam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\n","    \n","optimizer = tf.train.AdamOptimizer() #tf v1\n","#optimizer = tf.optimizers.Adam()  #tf v2\n","\n","#prepare checkpoint details\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Checkpoint file is missing\")\n","\n","\n","print(\"ChatBot: Hello there !\")\n","while (True):\n","    question = input(\"You: \")\n","    if question.lower() == 'goodbye':\n","        print(\"ChatBot: Bye bye, see you soon!\")\n","        break\n","    if embedding_type == 'word':\n","        #takes care for char and mix\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        cleaned_input = preprocess_sentence_char(question)\n","   \n","    if len(cleaned_input.strip()):\n","        \n","        reply, score = QAM3_beam_search_prediction(cleaned_input, rnntype, qam3_question_encoder, qam3_answer_decoder,  qam3_question_encoder, qam3_answer_encoder, qam3_label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,beam_size)    \n"," \n","        reply = re.sub('<start>', '', reply)\n","        idx = reply.find('<end>')\n","        if idx > 0:\n","            reply = reply[:idx]\n","        else:\n","            reply = re.sub('<end>', '', reply)\n","        reply = re.sub('<pad>', '', reply)\n","        print('Chatbot: ' + reply)\n","    else:\n","        reply = 'Sorry, I could not under stand anything you said.'\n","        score=-1\n","        print('Chatbot: ' + reply)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Reloading existing checkpoint file\n","Existing checkpoint file loaded\n","ChatBot: Hello there !\n","You: Hi, how are you ?\n","Chatbot:  how are you staying away \n","You: anybody know what today for a great day is ?\n","Chatbot:  to take your dogs on a walk with your girlfriend \n","You: wait are you coming home too ?\n","Chatbot:  from the first dance \n","You: wow u ! so talented \n","Chatbot:  thank you \n","You: why ? tech companies in need of diverse teams and people in need of can help each other .\n","Chatbot:  cool ! lmk if you could use a freelance professor -- filled out the volunteer \n","You: takes one to know one\n","Chatbot:  yes does but you def win \n","You: happy birthday\n","Chatbot:  awesome thank you so much i appreciate it \n","You: should i trade spencer ware for jordan matthews standard scoring . season long\n","Chatbot:  ok with that ! \n","You: how are you doing ?\n","Chatbot:  lol im at the bus stop \n","You: where are you now ?\n","Chatbot:  new york university \n","You: what are you doing there ?\n","Chatbot:  an installation at 10 pm est ? \n"],"name":"stdout"}]}]}