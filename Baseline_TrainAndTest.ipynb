{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_TrainAndTest.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hM9pm7k33Fbj","colab_type":"code","cellView":"form","colab":{}},"source":["#@title\n","#taken from \n","#https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n","#adapted for chatbot\n","#orignal code was for translation"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VC47FnQTN-fd","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"61449b77-749d-4cb7-92e6-e90049fe6703","executionInfo":{"status":"ok","timestamp":1573451808530,"user_tz":-480,"elapsed":3043,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","from __future__ import absolute_import, division, print_function\n","# Import TensorFlow >= 1.10 and enable eager execution\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","tf.enable_eager_execution()\n","print(tf.__version__)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FbburUIpDSNu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"a41fccd5-b502-4fea-ab89-dfdf786e4ec6","executionInfo":{"status":"ok","timestamp":1573451839492,"user_tz":-480,"elapsed":34001,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2YbGaAh8IvqD","colab_type":"code","cellView":"both","colab":{}},"source":["#@title\n","#path to settings file\n","#get input from user\n","dataset_folder = '/content/gdrive/My Drive/ChatBot_Dataset/narrative_qa/'\n","settings_file_path = dataset_folder + 'settings_baseline.json'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHx6xH8ZWWcn","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"55e58e28-e32f-497a-a5ff-f1dbedb23105","executionInfo":{"status":"ok","timestamp":1573451841825,"user_tz":-480,"elapsed":36325,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","import json\n","import unicodedata\n","import re\n","from datetime import datetime\n","from __future__ import absolute_import, division, print_function\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import os\n","import time\n","\n","class Settings():\n","    def __init__ (self,settings_file_path,parent_folder):\n","        self.settings = settings_file_path\n","        self.read_settings(parent_folder )\n","\n","    def read_settings(self,parent_folder):\n","        with open(self.settings) as f:\n","            jsonsettings = json.load(f)\n","            \n","        self.model_path = parent_folder + jsonsettings[\"model_path\"]\n","        self.qam3_path = parent_folder + jsonsettings[\"qam3_path\"]\n","        self.coreref_path = parent_folder + jsonsettings[\"coreref_path\"]\n","       \n","        self.training_set_path = parent_folder + jsonsettings[\"training_set_path\"]\n","        self.validation_set_path = parent_folder + jsonsettings[\"validation_set_path\"]\n","        self.test_set_path = parent_folder + jsonsettings[\"test_set_path\"]\n","        self.training_size = jsonsettings[\"training_size\"]\n","        self.validation_size = jsonsettings[\"validation_size\"]\n","        \n","        self.embedding_type = jsonsettings[\"embedding_type\"]\n","        self.encoder_type = jsonsettings[\"encoder_type\"]\n","        self.rnn_type = jsonsettings[\"rnn_type\"]\n","        self.internal_feedback = jsonsettings[\"internal_feedback\"]\n","        self.dropout = jsonsettings[\"dropout\"]\n","        self.recurrent_dropout = jsonsettings[\"recurrent_dropout\"]\n","        \n","        self.checkpoint_path = self.model_path + '/' + jsonsettings[\"checkpoint_path\"]\n","        self.checkpoint_path_qam3 = self.qam3_path + '/' + jsonsettings[\"checkpoint_path\"]\n","        self.checkpoint_path_coreref = self.coreref_path + '/' + jsonsettings[\"checkpoint_path\"]\n","\n","        self.embedding_dim = jsonsettings[\"embedding_dim\"]\n","        self.units = jsonsettings[\"units\"]\n","        self.epochs = jsonsettings[\"epochs\"]\n","        self.batch_size = jsonsettings[\"batch_size\"]\n","        self.beam_size = jsonsettings[\"beam_size\"]\n","\n","        self.input_vocab_path = parent_folder  + jsonsettings[\"input_vocab_path\"]\n","        self.input_vocab_size = jsonsettings[\"input_vocab_size\"]\n","        self.output_vocab_path = parent_folder  + jsonsettings[\"output_vocab_path\"]\n","        self.output_vocab_size = jsonsettings[\"output_vocab_size\"]\n","        self.label_vocab_path_qam3 = parent_folder + jsonsettings[\"label_vocab_path_qam3\"]\n","        self.label_vocab_size_qam3 = jsonsettings[\"label_vocab_size_qam3\"]\n","        self.label_vocab_path_coreref = parent_folder + jsonsettings[\"label_vocab_path_coreref\"]\n","        self.label_vocab_size_coreref = jsonsettings[\"label_vocab_size_coreref\"]\n","\n","        self.input_max_len = jsonsettings[\"input_max_len\"]\n","        self.output_max_len = jsonsettings[\"output_max_len\"]\n","        self.label_max_len = jsonsettings[\"label_max_len\"]\n","        \n","        self.training_loss_path = self.model_path + '/' + jsonsettings[\"training_loss_path\"]\n","        self.validation_loss_path = self.model_path + '/' + jsonsettings[\"validation_loss_path\"]\n","        self.predictions_path = self.model_path + '/' + jsonsettings[\"predictions_path\"]\n","\n","    def update_settings(self,jsonfilepath,key,value):\n","        jsonFile = open(jsonfilepath, \"r\") # Open the JSON file for reading\n","        data = json.load(jsonFile) # Read the JSON into the buffer\n","        jsonFile.close() # Close the JSON file\n","    \n","        data[key] = str(value)\n","    \n","        ## Save our changes to JSON file\n","        jsonFile = open(jsonfilepath, \"w\")\n","        jsonFile.write(json.dumps(data))\n","        jsonFile.close()\n","        \n","        \n","# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","def removelastquote(word):\n","    if len(word) > 1:\n","        if (( word[0]) == \"'\"):\n","           word = word [1:]\n","        if (( word[len(word)-1]) == \"'\"):\n","           word = word [0:-1]\n","        return word\n","    else:\n","        return word\n","      \n","def clean_string(line):\n","    line = line.lower()\n","    #line = change_quote(line)\n","    #line = line.replace(\"'\",\"\")\n","    #line = line.replace(\"`\",\"'\")\n","    line = re.sub(r\"[^a-z0123456789'!?,.-]+\", \" \", line)\n","    line = re.sub(r'[\"?\"]+', \" ? \", line)\n","    line = re.sub(r'[\".\"]+', \" . \", line)\n","    line = re.sub(r'[\",\"]+', \" , \", line)\n","    line = re.sub(r'[\"!\"]+', \" ! \", line)\n","    line = re.sub(r'[\" \"]+', \" \", line)\n","    return line.strip()\n","\n","def preprocess_sentence_word(sentence):\n","    sentence = clean_string(sentence)  \n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    sentence = '<start> ' + sentence + ' <end>'\n","    return sentence\n","\n","def preprocess_user_input_word(sentence,input_vocab):\n","    sentence = clean_string(sentence)  \n","    #to do: may be try adding a <unk> token instead\n","    new_sentence=\"\"\n","    for word in sentence.split(' '):\n","        if word in input_vocab.idx2word.values():\n","            new_sentence = new_sentence + word + ' '\n","        else:\n","            new_sentence = new_sentence + '<unk>' + ' '\n","    #new_sentence = re.sub(r'[\" \"]+', \" \", new_sentence)\n","    new_sentence = new_sentence.strip()\n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    new_sentence = '<start> ' + new_sentence + ' <end>'\n","    #print(new_sentence)\n","    return new_sentence\n","\n","def reverse_sentence(sentence):\n","    wordlist=\"\"\n","    wordlist=sentence.split(\" \")\n","    reversed_wordlist=wordlist[-1::-1]\n","    reversed_str= ' '.join(reversed_wordlist)\n","    return reversed_str\n","\n","def get_dataset_size(path):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    return len(lines)\n","\n","def create_dataset_word(path, num_examples):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    word_pairs = [[preprocess_sentence_word(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","    return word_pairs\n","\n","class EmbeddingIndex():\n","  def __init__(self, sentence):\n","    self.sentence = sentence\n","    self.word2idx = {}\n","    self.idx2word = {}\n","    self.vocab = set()\n","    \n","    self.create_index()\n","    \n","  def create_index(self):\n","    for phrase in self.sentence:\n","      self.vocab.update(phrase.split(' '))\n","    \n","    self.vocab = sorted(self.vocab)\n","    \n","    self.word2idx['<pad>'] = 0\n"," \n","    for index, word in enumerate(self.vocab):\n","        self.word2idx[word] = index + 1\n","    \n","    for word, index in self.word2idx.items():\n","      self.idx2word[index] = word\n","\n","      \n","def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def remove_last_empty_line(path):\n","    with open(path) as f_input:\n","        data = f_input.read().rstrip('\\n')\n","    with open(path, 'w') as f_output:    \n","        f_output.write(data)\n","        \n","def remove_empty_lines(path):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    with open(path, 'w') as f:   \n","        for line in lines:\n","            if not line.strip(): \n","                continue  # skip the empty line\n","            else:\n","                 f.write(\"{}\\n\".format(line))\n","        \n","def get_vocab(path):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    vocab = EmbeddingIndex(word for word in lines)\n","    return vocab\n","\n","def load_dataset_word(path, num_examples, input_vocab, output_vocab):\n","    # creating cleaned input, output pairs\n","    pairs = create_dataset_word(path, num_examples)\n","\n","    # Input sentences\n","    input_tensor = [[input_vocab.word2idx[word] for word in input_sentence.split(' ')] for input_sentence, output_sentence in pairs]\n","\n","    # Reverse Input sentences (as suggested in sutskever)\n","    #comment this out for now\n","    #reversed_input_tensor = [[input_vocab.word2idx[word] for word in reverse_sentence(input_sentence).split(' ')] for input_sentence, output_sentence in pairs]\n","    \n","    # Output sentences\n","    target_tensor = [[output_vocab.word2idx[word] for word in output_sentence.split(' ')] for input_sentence, output_sentence in pairs]\n","    \n","    # Calculate max_length of input and output tensor\n","    # Here, we'll set those to the longest sentence in the dataset\n","    #max_length_inp, max_length_reversed_input, max_length_tar = max_length(input_tensor), max_length(reversed_input_tensor), max_length(target_tensor)\n","    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n","    \n","    # Padding the input and output tensor to the maximum length\n","    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen=max_length_inp,  padding='post')\n","    \n","    #reversed_input_tensor = tf.keras.preprocessing.sequence.pad_sequences(reversed_input_tensor,  maxlen=max_length_reversed_input, padding='post')\n","    \n","    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen=max_length_tar,  padding='post')\n","    \n","    #return input_tensor, reversed_input_tensor, target_tensor, max_length_inp, max_length_reversed_input, max_length_tar     \n","    return input_tensor, target_tensor, max_length_inp, max_length_tar     \n","\n","def log_epoch_loss(path,epoch, loss):\n","    now=datetime.today().isoformat()\n","    with open(path, 'a') as log_file:\n","        log_file.write(\"{},{},{}\\n\".format(now,epoch,loss))\n","        \n","#MODEL\n","def gru(units,dropout, recurrent_dropout,go_backwards=False):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","  if tf.test.is_gpu_available():\n","    return tf.keras.layers.CuDNNGRU(units, \n","                                    return_sequences=True, \n","                                    return_state=True, \n","                                    recurrent_initializer='glorot_uniform',\n","                                    go_backwards=go_backwards)\n","  else:\n","    return tf.keras.layers.GRU(units, \n","                               return_sequences=True, \n","                               return_state=True, \n","                               dropout=dropout,\n","                               recurrent_dropout=recurrent_dropout,\n","                               recurrent_activation='sigmoid', \n","                               recurrent_initializer='glorot_uniform',\n","                               go_backwards=go_backwards)\n","    \n","def lstm(units,dropout, recurrent_dropout,go_backwards=False):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","  #dropout not supported in GPU\n","  if tf.test.is_gpu_available():\n","    return tf.keras.layers.CuDNNLSTM(units, \n","                                    return_sequences=True, \n","                                    return_state=True, \n","                                    recurrent_initializer='glorot_uniform',\n","                                    go_backwards=go_backwards)\n","  else:\n","    return tf.keras.layers.LSTM(units, \n","                               return_sequences=True, \n","                               return_state=True,\n","                               dropout=dropout,\n","                               recurrent_dropout=recurrent_dropout,\n","                               recurrent_activation='sigmoid', \n","                               recurrent_initializer='glorot_uniform',\n","                               go_backwards=go_backwards)\n","  \n","class Encoder_BidirectionalGRU(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout,recurrent_dropout):\n","        super(Encoder_BidirectionalGRU, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru_fw = gru(self.enc_units,dropout,recurrent_dropout)\n","        self.gru_bw = gru(self.enc_units,dropout,recurrent_dropout,True)\n","        \n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output_fw, state_fw = self.gru_fw(x, initial_state = hidden)        \n","        output_bw, state_bw = self.gru_bw(x, initial_state = hidden)  \n","        output =  tf.concat([output_fw,output_bw], axis=2)\n","        state =  tf.concat([state_fw,state_bw], axis=1)\n","        return output, state, x\n","    \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","        \n","#class Encoder_BidirectionalGRU(tf.keras.Model):\n","#    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout,recurrent_dropout):\n","#        super(Encoder_BidirectionalGRU, self).__init__()\n","#        self.batch_sz = batch_sz\n","#        self.enc_units = enc_units\n","#        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","#        self.gru_fw = gru(self.enc_units,dropout,recurrent_dropout)\n","#        self.gru_bw = gru(self.enc_units,dropout,recurrent_dropout,True)\n","        \n","#    def call(self, x, hidden):\n","#        x = self.embedding(x)\n","#        output_fw, state_fw = self.gru_fw(x, initial_state = hidden)        \n","#        output_bw, state_bw = self.gru_bw(x, initial_state = hidden)  \n","#        output =  tf.concat([output_fw,output_bw], axis=2)\n","#        state =  tf.concat([state_fw,state_bw], axis=1)\n","#        return output, state\n","    \n","#    def initialize_states(self):\n","#        return tf.zeros((self.batch_sz, self.enc_units))\n","      \n","\n","class Encoder_BidirectionalLSTM(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout,recurrent_dropout):\n","        super(Encoder_BidirectionalLSTM, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm_fw = lstm(self.enc_units,dropout,recurrent_dropout)\n","        self.lstm_bw = lstm(self.enc_units,dropout,recurrent_dropout,True)\n","        \n","    def call(self, x, hidden, cell_state):\n","        x = self.embedding(x)\n","        output_fw, state_fw, cell_state_fw = self.lstm_fw(x, initial_state = [hidden, cell_state])        \n","        output_bw, state_bw, cell_state_bw = self.lstm_bw(x, initial_state = [hidden, cell_state])        \n","        output =  tf.concat([output_fw,output_bw], axis=2)\n","        state =  tf.concat([state_fw,state_bw], axis=1)\n","        state =  tf.concat([cell_state_fw,cell_state_bw], axis=1)\n","        return output, state, cell_state\n","   \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","\n","        \n","class Decoder_GRU(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout,recurrent_dropout):\n","        super(Decoder_GRU, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.dec_units,dropout,recurrent_dropout)\n","           \n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","               \n","        # used for attention (Bahdanau style)\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, enc_output, hidden):\n","\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, hidden_size)\n","        #Bahdanau's additive style (Bahdanau attention)        \n","\n","        #original was tanh\n","        #score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        score = tf.nn.relu6(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","\n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","            \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","            \n","        #return x, state, attention_weights\n","        return x, state\n"," \n","       \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))\n","         \n","class Decoder_LSTM(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, dropout,recurrent_dropout):\n","        super(Decoder_LSTM, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = lstm(self.dec_units,dropout,recurrent_dropout)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","               \n","        # used for attention (Bahdanau style)\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, enc_output, hidden, cell_state):\n","\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, hidden_size)\n","        #Bahdanau's additive style (Bahdanau attention)        \n","\n","        #original was tanh\n","        #score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        score = tf.nn.relu6(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying score to self.V\n","        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the LSTM\n","        output, state, cell_state = self.lstm(x)\n","\n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","            \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","            \n","        #return x, state, attention_weights\n","        return x, state, cell_state\n","           \n","       \n","    def initialize_states(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))\n","\n","def generate_samples(input_file, sample_file,lines_count, samples_count):\n","    import itertools\n","    import random\n","\n","    def random_gen(low, high):\n","        while True:\n","            yield random.randrange(low, high)\n","\n","    gen = random_gen(1, lines_count)\n","\n","    items = set()\n","\n","    # try to add elem to set until set length is less than 10\n","    for x in itertools.takewhile(lambda x: len(items) < samples_count+1, gen): \n","        items.add(x)\n","    #print(items)\n","  \n","    with open(sample_file, 'w') as f:\n","        lines = open(input_file, encoding='UTF-8', errors='ignore').read().strip().split('\\n')\n","        line_number=0\n","        for line in lines:\n","            line_number  += 1\n","            if line_number in items:\n","                f.write(\"{}\\n\".format(line))\n","    \n","    remove_empty_lines(sample_file)\n","    remove_last_empty_line(sample_file)\n","    \n","def get_samples(path):\n","    questions=[]\n","    answers=[]\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    lines2= filter(lambda x: not x.isspace(), lines)\n","    for line in lines2:\n","        questions.append(line.split('\\t')[0])\n","        answers.append(line.split('\\t')[1])\n","    return questions, answers\n","\n","def loss_function(real, pred):\n","  mask = 1 - np.equal(real, 0)\n","  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n","  return tf.reduce_mean(loss_)\n","\n","def get_test_questions(path, start_row, num_examples):\n","    lines = open(path, encoding='UTF-8',errors='ignore').read().strip().split('\\n')\n","    end_row=start_row+num_examples\n","    word_pairs = [l.split('\\t') for l in lines[start_row:end_row]]\n","    return word_pairs\n","\n","def greedy_search_prediction(question, rnntype, encoder, decoder, input_vocab, output_vocab, input_max_len, output_max_len):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result = ''\n","\n","    hidden = encoder.initialize_states()\n","    if rnntype == 'LSTM':\n","        enc_cell = encoder.initialize_states()\n","        dec_cell = decoder.initialize_states()\n","        enc_out, enc_hidden, enc_cell = encoder(inputs, hidden, enc_cell)\n","    else:\n","        enc_out, enc_hidden = encoder(inputs, hidden)\n","        \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","\n","    #print(output_vocab.idx2word)\n","    score_highest=[]\n","    #score_lowest=[]\n","    #score_mean=[]\n","    #score_total=[]\n","    \n","    for t in range(output_max_len):\n","      \n","        if rnntype == 'LSTM':\n","          #predictions, dec_hidden, dec_cell, attention_weights = decoder(dec_input, enc_out, dec_hidden, dec_cell )\n","          predictions, dec_hidden, dec_cell = decoder(dec_input, enc_out, dec_hidden, dec_cell )\n","        else:\n","          #predictions, dec_hidden, attention_weights = decoder(dec_input, enc_out, dec_hidden)\n","          predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","        #print(\"Predictions is: {}\".format(predictions))\n","    \n","        #predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        \n","        highest = (tf.reduce_max(predictions[0])).numpy()\n","        #lowest  = (tf.reduce_min(predictions[0])).numpy()\n","        #mean    = (tf.reduce_mean(predictions[0])).numpy()\n","        #total   = (tf.reduce_sum(predictions[0])).numpy()\n","      \n","        score_highest.append(highest)\n","        #score_lowest.append(lowest)        \n","        #score_mean.append(mean)        \n","        #score_total.append(total) \n","        \n","        result += output_vocab.idx2word[predicted_id] + ' '\n","        \n","        if output_vocab.idx2word[predicted_id] == '<end>':\n","            #return result, sentence, attention_plot\n","            high_score = sum(score_highest)/(t+1)\n","            #low_score = sum(score_lowest)/(t+1)    \n","            #mean_score = sum(score_mean)/(t+1)\n","            #total_score = sum(score_total)/(t+1)    \n","            #bleu_score = calculate_bleu(clean_string(reference_answer),clean_string(result))\n","            return result, high_score\n","        \n","            # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","    \n","    #return result, sentence, attention_plot\n","    high_score = sum(score_highest)/(t+1)\n","    #low_score = sum(score_lowest)/(t+1)    \n","    #mean_score = sum(score_mean)/(t+1)    \n","    #total_score = sum(score_total)/(t+1)   \n","    #bleu_score = calculate_bleu(reference_answer,result)    \n","    \n","    return result, high_score\n","\n","def get_k_top_predictions(predictions,avg_sent_prob ,predicted_sentence, dec_input,k=1):\n","    #the reason sentence_prob,predicted_sentence, dec_input is passed is that i can use it back unchanged\n","    #there is no processing done for these values bcos i need it unchnaged in the calling function\n","    k_top_predictions = tf.nn.top_k(predictions[0],k)\n","    predictedids = k_top_predictions[1].numpy()\n","    probabilities = k_top_predictions[0].numpy()\n","    #create is, prob pair of list\n","    id_prob_pair=[]\n","    for idx in range(k):\n","        id_prob_pair.append([predictedids[idx],probabilities[idx],avg_sent_prob ,predicted_sentence, dec_input])\n","    return id_prob_pair\n","    \n","def beam_search_prediction(question, rnntype, encoder, decoder, input_vocab, output_vocab, input_max_len, output_max_len,k=1):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    enc_out, enc_hidden, _ = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    #idea from https://github.com/mmehdig/lm_beam_search/blob/master/beam_search.py\n","    #K_beam[prob,result,dec_input,dec_hidden]\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    \n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","            \n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    #predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","                    predicted_sentence += '<end>' \n","                    total_sent_prob += word_prob\n","                    dec_input = tf.expand_dims([predicted_id], 0)\n","                    all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","                    total_sent_prob += word_prob\n","                    dec_input = tf.expand_dims([predicted_id], 0)\n","                    all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","            #k_beam = sorted(all_k_beams)[:k]\n","            predicted_sent_tmp = k_beam[0][1]\n","            if '<end>' in predicted_sent_tmp:\n","                avg_sent_prob = (k_beam[0][0])/(t+1)\n","                return predicted_sent_tmp, avg_sent_prob\n","\n","    #get top 1 for now\n","    t_beam = sorted(k_beam,reverse=True)[:1]\n","    #t_beam = sorted(k_beam)[:1]\n","    #sentence_prob_pair=[]\n","    #for sentence_prob, total_sent_prob, predicted_sentence, _ , _ in t_beam:\n","    #    avg_sent_prob = total_sent_prob/t\n","    #    sentence_prob_pair.append([sentence_prob, avg_sent_prob, predicted_sentence])\n","\n","    predicted_sent = t_beam[0][1]\n","    #if t==0:\n","    #    avg_sent_prob = 0\n","    #else:\n","    #need to get average\n","    avg_sent_prob = (t_beam[0][0])/(t+1)\n","    return predicted_sent, avg_sent_prob\n","\n","print(\"Helper Functions Done\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Helper Functions Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qvsXFdRK2XzU","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"a287e95a-1cfd-4cc8-b4f6-5f169ceea0b0","executionInfo":{"status":"ok","timestamp":1573451843169,"user_tz":-480,"elapsed":37665,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","#Variables' definitions\n","\n","param = Settings(settings_file_path,dataset_folder)\n","corpus_path = param.training_set_path#input from user\n","param = Settings(settings_file_path,dataset_folder)\n","training_set_path = param.training_set_path\n","validation_set_path = param.validation_set_path\n","input_vocab_path=param.input_vocab_path\n","output_vocab_path=param.output_vocab_path\n","checkpoint_path = param.checkpoint_path\n","epochs = int(param.epochs)\n","batch_size = int(param.batch_size)\n","embedding_dim = int(param.embedding_dim)\n","units  = int(param.units)\n","training_loss_path = param.training_loss_path\n","validation_loss_path = param.validation_loss_path\n","encoder_type=param.encoder_type\n","embedding_type=param.embedding_type\n","rnn_type=param.rnn_type\n","training_size = int(param.training_size)\n","validation_size = int(param.validation_size)\n","dropout=float(param.dropout)\n","recurrent_dropout=float(param.recurrent_dropout)\n","internal_feedback=param.internal_feedback\n","test_questions_path=param.test_set_path\n","predictions_path=param.predictions_path\n","input_max_len = int(param.input_max_len)\n","output_max_len = int(param.output_max_len)\n","beam_size=int(param.beam_size)\n","rnntype=rnn_type.upper()\n","input_vocab = get_vocab(input_vocab_path)\n","output_vocab = get_vocab(output_vocab_path)\n","input_vocab_size = len(input_vocab.word2idx)\n","output_vocab_size = len(output_vocab.word2idx)\n","rnntype=rnn_type.upper()\n","\n","print('Model Details')\n","print('Training size is {}'.format(training_size))\n","print(\"Vocab size is {}:{}\".format(input_vocab_size,output_vocab_size))\n","print('Epoch count is {}'.format(epochs))\n","print('Embedding size is {}'.format(embedding_dim))\n","print('Hidden units is {}'.format(units))\n","\n","#print('Embedding type: {}'.format(embedding_type))\n","#print('Encoder type is {}'.format(encoder_type))\n","#print('RNN type is {}'.format(rnntype))\n","#print('Dropout rate is {}'.format(dropout))\n","#print('Feedback mechanism {}'.format(internal_feedback))\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Model Details\n","Training size is 24000\n","Vocab size is 17303:18866\n","Epoch count is 20\n","Embedding size is 512\n","Hidden units is 512\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sF8xyEluWAXm","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"29758d57-26a1-41f4-807d-7cc2d7484968","executionInfo":{"status":"ok","timestamp":1573451843171,"user_tz":-480,"elapsed":37663,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","\"\"\"\n","#Training\n","input_tensor, output_tensor, input_max_len, output_max_len = load_dataset_word(training_set_path, training_size, input_vocab, output_vocab)\n"," \n","if rnntype == 'GRU':\n","    baseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","    baseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","    \n","\n","optimizer = tf.train.AdamOptimizer() #tf v1\n","#optimizer = tf.optimizers.Adam()  #tf v2\n","\n","#prepare checkpoint details\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)\n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Creating new checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)\n","  \n","#floor division to return only integer\n","outer_loop = epochs #// 10\n","inner_loop = 1\n","start = time.time()\n","\n","for loop in range(outer_loop):\n","\n","  #Prepare data for training and validation\n","  input_tensor_train, input_tensor_val, output_tensor_train, output_tensor_val = train_test_split(input_tensor, output_tensor, test_size=0.2)\n","    \n","  buffer_size = len(input_tensor_train)\n","  total_batches = buffer_size//batch_size\n","  dataset_train = tf.data.Dataset.from_tensor_slices((input_tensor_train, output_tensor_train)).shuffle(buffer_size)\n","  dataset_train = dataset_train.batch(batch_size, drop_remainder=True)\n","\n","  buffer_size_val = len(input_tensor_val)\n","  total_batches_val = buffer_size_val//batch_size\n","  dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, output_tensor_val)).shuffle(buffer_size_val)\n","  dataset_val = dataset_val.batch(batch_size, drop_remainder=True)\n","\n","  #Training\n","  print(\"Seq2Seq #{} Training Started\".format(loop))\n","\n","  #start = time.time()\n","  loss = 0\n","  total_loss = 0\n","  avg_loss = 0\n","  prev_avg_loss=1000000000\n","\n","  #Overwrite epochs here for quick testing\n","  #epochs=1 \n","\n","  for epoch in range(inner_loop):\n","      #start = time.time()\n","\n","      hidden = baseline_question_encoder.initialize_states()\n","\n","      total_loss = 0\n","\n","      #for (batch, (inp, inp2, targ)) in enumerate(dataset):\n","      for (batch, (inp, targ)) in enumerate(dataset_train):\n","          loss = 0\n","          #print('batch # {}'.format(batch))\n","          with tf.GradientTape() as tape:\n","              input_tensors=[]\n","              input_tensors.append(inp)\n","\n","              enc_output, enc_hidden, _ = baseline_question_encoder(inp, hidden)\n","\n","              dec_hidden = enc_hidden\n","              dec_input = tf.expand_dims([output_vocab.word2idx['<start>']] * batch_size, 1)       \n","\n","              for t in range(1, targ.shape[1]):\n","                  # passing enc_output to the decoder\n","\n","                  predictions, dec_hidden = baseline_answer_decoder(dec_input, enc_output, dec_hidden)\n","                  #print(predictions)\n","                  loss += loss_function(targ[:, t], predictions)\n","\n","                  # using teacher forcing\n","                  #try disable teacher forcing \n","                  dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","          batch_loss = (loss / int(targ.shape[1]))\n","          #print(\"Batch:{}\".format(batch))\n","          total_loss += batch_loss\n","          variables = baseline_question_encoder.variables + baseline_answer_decoder.variables\n","          gradients = tape.gradient(loss, variables)\n","          #cmd below changes the weights of the encoder and decoder\n","          optimizer.apply_gradients(zip(gradients, variables))\n","          #checkpoint.save(file_prefix = checkpoint_prefix)\n","          #print(\"Checkpoint saved\")\n","\n","      # saving (checkpoint) the model every 100 epochs\n","      #print('Epoch {} : Total loss {}'.format(epoch+1, total_loss.numpy()))\n","      avg_loss=total_loss.numpy()/total_batches\n","      print('Epoch {} : Training loss {}'.format(loop, avg_loss))\n","      log_epoch_loss(training_loss_path, loop, avg_loss)\n","      #epoch1=epoch+1\n","      #if epoch1 % 10==0:\n","      checkpoint.save(file_prefix = checkpoint_prefix)\n","      print(\"Checkpoint saved\")\n","\n","  #Validation\n","\n","  print(\"Seq2Seq #{} Validation Started\".format(loop))\n","\n","  #print(\"Reloading existing checkpoint file\")\n","  #checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n","  #checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  #print(\"Existing checkpoint file loaded\")\n","\n","  #start = time.time()\n","  loss = 0\n","  total_loss = 0\n","  avg_loss = 0\n","  prev_avg_loss=1000000000\n","\n","  validation_loop=1\n","\n","  for epoch in range(validation_loop):\n","      #start = time.time()\n","\n","      hidden = baseline_question_encoder.initialize_states()\n","      total_loss = 0\n","\n","      #for (batch, (inp, inp2, targ)) in enumerate(dataset):\n","      for (batch, (inp, targ)) in enumerate(dataset_val):\n","          loss = 0\n","          #print('batch # {}'.format(batch))\n","          input_tensors=[]\n","          input_tensors.append(inp)\n","\n","          enc_output, enc_hidden, _ = baseline_question_encoder(inp, hidden)\n","\n","          dec_hidden = enc_hidden\n","          dec_input = tf.expand_dims([output_vocab.word2idx['<start>']] * batch_size, 1)       \n","\n","          for t in range(1, targ.shape[1]):\n","              # passing enc_output to the decoder\n","\n","              predictions, dec_hidden = baseline_answer_decoder(dec_input, enc_output, dec_hidden)\n","              #print(predictions)\n","              loss += loss_function(targ[:, t], predictions)\n","\n","              dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","          batch_loss = (loss / int(targ.shape[1]))\n","          #print(\"Batch:{}\".format(batch))\n","          total_loss += batch_loss\n","\n","      # saving (checkpoint) the model every 100 epochs\n","      #print('Epoch {} : Total loss {}'.format(epoch+1, total_loss.numpy()))\n","      avg_loss=total_loss.numpy()/total_batches_val\n","      print('Epoch {} : Validation loss {}'.format(loop, avg_loss))\n","      log_epoch_loss(validation_loss_path, loop, avg_loss)\n","  \n","end_time = time.time()\n","print('Total time taken for {} epochs is {} secs'.format(epochs, end_time - start))\n","print('Higher Loop Done')\n","\"\"\""],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#Training\\ninput_tensor, output_tensor, input_max_len, output_max_len = load_dataset_word(training_set_path, training_size, input_vocab, output_vocab)\\n \\nif rnntype == \\'GRU\\':\\n    baseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\n    baseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\n    \\n\\noptimizer = tf.train.AdamOptimizer() #tf v1\\n#optimizer = tf.optimizers.Adam()  #tf v2\\n\\n#prepare checkpoint details\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\ncheckpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\\n\\nif os.path.exists(checkpoint_filepath):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)\\n  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n  print(\"Existing checkpoint file loaded\")\\nelse:\\n  print(\"Creating new checkpoint file\")\\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)\\n  \\n#floor division to return only integer\\nouter_loop = epochs #// 10\\ninner_loop = 1\\nstart = time.time()\\n\\nfor loop in range(outer_loop):\\n\\n  #Prepare data for training and validation\\n  input_tensor_train, input_tensor_val, output_tensor_train, output_tensor_val = train_test_split(input_tensor, output_tensor, test_size=0.2)\\n    \\n  buffer_size = len(input_tensor_train)\\n  total_batches = buffer_size//batch_size\\n  dataset_train = tf.data.Dataset.from_tensor_slices((input_tensor_train, output_tensor_train)).shuffle(buffer_size)\\n  dataset_train = dataset_train.batch(batch_size, drop_remainder=True)\\n\\n  buffer_size_val = len(input_tensor_val)\\n  total_batches_val = buffer_size_val//batch_size\\n  dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, output_tensor_val)).shuffle(buffer_size_val)\\n  dataset_val = dataset_val.batch(batch_size, drop_remainder=True)\\n\\n  #Training\\n  print(\"Seq2Seq #{} Training Started\".format(loop))\\n\\n  #start = time.time()\\n  loss = 0\\n  total_loss = 0\\n  avg_loss = 0\\n  prev_avg_loss=1000000000\\n\\n  #Overwrite epochs here for quick testing\\n  #epochs=1 \\n\\n  for epoch in range(inner_loop):\\n      #start = time.time()\\n\\n      hidden = baseline_question_encoder.initialize_states()\\n\\n      total_loss = 0\\n\\n      #for (batch, (inp, inp2, targ)) in enumerate(dataset):\\n      for (batch, (inp, targ)) in enumerate(dataset_train):\\n          loss = 0\\n          #print(\\'batch # {}\\'.format(batch))\\n          with tf.GradientTape() as tape:\\n              input_tensors=[]\\n              input_tensors.append(inp)\\n\\n              enc_output, enc_hidden, _ = baseline_question_encoder(inp, hidden)\\n\\n              dec_hidden = enc_hidden\\n              dec_input = tf.expand_dims([output_vocab.word2idx[\\'<start>\\']] * batch_size, 1)       \\n\\n              for t in range(1, targ.shape[1]):\\n                  # passing enc_output to the decoder\\n\\n                  predictions, dec_hidden = baseline_answer_decoder(dec_input, enc_output, dec_hidden)\\n                  #print(predictions)\\n                  loss += loss_function(targ[:, t], predictions)\\n\\n                  # using teacher forcing\\n                  #try disable teacher forcing \\n                  dec_input = tf.expand_dims(targ[:, t], 1)\\n\\n          batch_loss = (loss / int(targ.shape[1]))\\n          #print(\"Batch:{}\".format(batch))\\n          total_loss += batch_loss\\n          variables = baseline_question_encoder.variables + baseline_answer_decoder.variables\\n          gradients = tape.gradient(loss, variables)\\n          #cmd below changes the weights of the encoder and decoder\\n          optimizer.apply_gradients(zip(gradients, variables))\\n          #checkpoint.save(file_prefix = checkpoint_prefix)\\n          #print(\"Checkpoint saved\")\\n\\n      # saving (checkpoint) the model every 100 epochs\\n      #print(\\'Epoch {} : Total loss {}\\'.format(epoch+1, total_loss.numpy()))\\n      avg_loss=total_loss.numpy()/total_batches\\n      print(\\'Epoch {} : Training loss {}\\'.format(loop, avg_loss))\\n      log_epoch_loss(training_loss_path, loop, avg_loss)\\n      #epoch1=epoch+1\\n      #if epoch1 % 10==0:\\n      checkpoint.save(file_prefix = checkpoint_prefix)\\n      print(\"Checkpoint saved\")\\n\\n  #Validation\\n\\n  print(\"Seq2Seq #{} Validation Started\".format(loop))\\n\\n  #print(\"Reloading existing checkpoint file\")\\n  #checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\\n  #checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n  #print(\"Existing checkpoint file loaded\")\\n\\n  #start = time.time()\\n  loss = 0\\n  total_loss = 0\\n  avg_loss = 0\\n  prev_avg_loss=1000000000\\n\\n  validation_loop=1\\n\\n  for epoch in range(validation_loop):\\n      #start = time.time()\\n\\n      hidden = baseline_question_encoder.initialize_states()\\n      total_loss = 0\\n\\n      #for (batch, (inp, inp2, targ)) in enumerate(dataset):\\n      for (batch, (inp, targ)) in enumerate(dataset_val):\\n          loss = 0\\n          #print(\\'batch # {}\\'.format(batch))\\n          input_tensors=[]\\n          input_tensors.append(inp)\\n\\n          enc_output, enc_hidden, _ = baseline_question_encoder(inp, hidden)\\n\\n          dec_hidden = enc_hidden\\n          dec_input = tf.expand_dims([output_vocab.word2idx[\\'<start>\\']] * batch_size, 1)       \\n\\n          for t in range(1, targ.shape[1]):\\n              # passing enc_output to the decoder\\n\\n              predictions, dec_hidden = baseline_answer_decoder(dec_input, enc_output, dec_hidden)\\n              #print(predictions)\\n              loss += loss_function(targ[:, t], predictions)\\n\\n              dec_input = tf.expand_dims(targ[:, t], 1)\\n\\n          batch_loss = (loss / int(targ.shape[1]))\\n          #print(\"Batch:{}\".format(batch))\\n          total_loss += batch_loss\\n\\n      # saving (checkpoint) the model every 100 epochs\\n      #print(\\'Epoch {} : Total loss {}\\'.format(epoch+1, total_loss.numpy()))\\n      avg_loss=total_loss.numpy()/total_batches_val\\n      print(\\'Epoch {} : Validation loss {}\\'.format(loop, avg_loss))\\n      log_epoch_loss(validation_loss_path, loop, avg_loss)\\n  \\nend_time = time.time()\\nprint(\\'Total time taken for {} epochs is {} secs\\'.format(epochs, end_time - start))\\nprint(\\'Higher Loop Done\\')\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"4oH2X0y0UTmO","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"e1a42154-140b-4176-9610-4f46cb0c48bd","executionInfo":{"status":"ok","timestamp":1573451843172,"user_tz":-480,"elapsed":37661,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","\n","\"\"\"\n","#greedy search\n","for question in questions:\n","    if embedding_type == 'word':\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        #takes care for char and mix\n","        cleaned_input = preprocess_sentence_char(question)\n","        \n","    #print(question)\n","    if len(cleaned_input.strip()):\n","        reply, score = greedy_search_prediction(cleaned_input, rnntype, encoder, decoder, input_vocab, output_vocab, input_max_len, output_max_len)    \n","        if embedding_type == 'char':\n","            reply = preprocess_output_char(reply)\n","        \n","        reply = re.sub('<start>', '', reply)\n","        reply = re.sub('<end>', '', reply)\n","        reply = re.sub('<pad>', '', reply)\n","        #print(reply)\n","        qa_pair.append(question.strip()+'\\t'+ reply.strip() + '\\t' + str(score))\n","    else:\n","        reply = 'Zero words/characters in vocabulary'\n","        score=-1\n","        qa_pair.append(question.strip()+'\\t'+ reply.strip() + '\\t' + str(score))\n","        \n","with open(predictions_path_gs, 'a') as f:\n","    for line in qa_pair:\n","        f.write(\"{}\\n\".format(line))\n","print('Greedy search done. File created at {} '.format(predictions_path_gs))\n","\"\"\"\n"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n#greedy search\\nfor question in questions:\\n    if embedding_type == \\'word\\':\\n        cleaned_input = preprocess_user_input_word(question,input_vocab)\\n    else:\\n        #takes care for char and mix\\n        cleaned_input = preprocess_sentence_char(question)\\n        \\n    #print(question)\\n    if len(cleaned_input.strip()):\\n        reply, score = greedy_search_prediction(cleaned_input, rnntype, encoder, decoder, input_vocab, output_vocab, input_max_len, output_max_len)    \\n        if embedding_type == \\'char\\':\\n            reply = preprocess_output_char(reply)\\n        \\n        reply = re.sub(\\'<start>\\', \\'\\', reply)\\n        reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        reply = re.sub(\\'<pad>\\', \\'\\', reply)\\n        #print(reply)\\n        qa_pair.append(question.strip()+\\'\\t\\'+ reply.strip() + \\'\\t\\' + str(score))\\n    else:\\n        reply = \\'Zero words/characters in vocabulary\\'\\n        score=-1\\n        qa_pair.append(question.strip()+\\'\\t\\'+ reply.strip() + \\'\\t\\' + str(score))\\n        \\nwith open(predictions_path_gs, \\'a\\') as f:\\n    for line in qa_pair:\\n        f.write(\"{}\\n\".format(line))\\nprint(\\'Greedy search done. File created at {} \\'.format(predictions_path_gs))\\n'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"GTn4KLbe1O0-","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"5d221b9b-d2f0-4816-8385-7f1556db5fb8","executionInfo":{"status":"ok","timestamp":1573452357366,"user_tz":-480,"elapsed":551851,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","\n","#Testing\n","batch_size=1   \n","\n","baseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","baseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","    \n","optimizer = tf.train.AdamOptimizer() #tf v1\n","#optimizer = tf.optimizers.Adam()  #tf v2\n","\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Checkpoint file is missing\")\n","\n","\n","#prepare checkpoint details\n","#checkpoint_metafilename='checkpoint'\n","#checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","#checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \n","#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","#print('Checkpoint path {}'.format(checkpoint_filepath))\n","\n","\n","start_row=0\n","test_size=1000\n","QA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\n","print('Questions path {}'.format(test_questions_path))\n","qa_pair=[]\n","\n","filename, file_extension = os.path.splitext(predictions_path)\n","beam_search_filename = filename + '_standard_bs_Baseline.txt'\n","print('File path: {}'.format(beam_search_filename))\n","#predictions_path_gs = filename + '_greedy.txt'\n","#predictions_path_bs = filename + '_bs_' + str(beam_size) + '.csv'\n","#just focus on beam search  \n","\n","\n","qa_pair_bs=[]\n","\n","#beam search\n","for question, answer in QA_Pairs:\n","\n","    #print(question)\n","    \n","    if embedding_type == 'word':\n","        #takes care for char and mix\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        cleaned_input = preprocess_sentence_char(question)\n","    \n","    if len(cleaned_input.strip()):\n","        reply,  score = beam_search_prediction(cleaned_input, rnntype, baseline_question_encoder, baseline_answer_decoder, input_vocab, output_vocab, input_max_len, output_max_len,beam_size)    \n","        \n","        if embedding_type == 'char':\n","            reply = preprocess_output_char(reply)\n"," \n","        reply = re.sub('<start>', '', reply)\n","        #reply = re.sub('<end>', '', reply)\n","        \n","        idx = reply.find('<end>')\n","        if idx > 0:\n","            reply = reply[:idx]\n","        else:\n","            reply = re.sub('<end>', '', reply)\n","        \n","        reply = re.sub('<pad>', '', reply)\n","        \n","        qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","    else:\n","        reply = 'Zero words/characters in vocabulary'\n","        score=-1\n","        qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        \n","with open(beam_search_filename, 'w') as f:\n","    for line in qa_pair_bs:\n","        f.write(\"{}\\n\".format(line))\n","\n","print('Standard beam search done. File created at {}'.format(beam_search_filename))\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Reloading existing checkpoint file\n","Existing checkpoint file loaded\n","Questions path /content/gdrive/My Drive/ChatBot_Dataset/narrative_qa/nqa_testing1k.txt\n","File path: /content/gdrive/My Drive/ChatBot_Dataset/narrative_qa/Baseline/predict_standard_bs_Baseline.txt\n","Standard beam search done. File created at /content/gdrive/My Drive/ChatBot_Dataset/narrative_qa/Baseline/predict_standard_bs_Baseline.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GiioA5eHJLgF","colab_type":"code","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"7d7822e0-2b8a-4849-d787-5346fb00fce0","executionInfo":{"status":"ok","timestamp":1573452357377,"user_tz":-480,"elapsed":551859,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["#@title\n","\n","#Enhanced Beam Search Common Functions\n","class SimilarityDecoder(tf.keras.Model):\n","    def __init__(self, vocab_size, units, dropout):\n","        super(SimilarityDecoder, self).__init__()\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, context_vector_question, context_vector_answer):\n","\n","        context_vector = context_vector_question * context_vector_answer\n","        \n","        x=self.fc(context_vector)\n","       \n","        return x\n","        \n","class SimilarityDecoder_RYANLOWE(tf.keras.Model):\n","    def __init__(self, vocab_size, units, dropout):\n","        super(SimilarityDecoder_RYANLOWE, self).__init__()\n","        \n","        self.M = tf.keras.layers.Dense(units*2)\n","        #self.fc = tf.keras.layers.Dense(vocab_size)\n","        self.fc = tf.keras.layers.Dense(vocab_size,activation='softmax')\n","        \n","    def call(self, context_vector_question, context_vector_answer):\n","\n","        predicted_question = self.M(context_vector_answer)\n","        dot_product = predicted_question * context_vector_question\n","        x=self.fc(dot_product)\n","        return x\n","  \n","def remove_everything_after_end_token(sentence):\n","    searchStr = '<end>'\n","    if sentence.find(searchStr)>0:\n","      return sentence[:sentence.find(searchStr)]\n","    else:\n","      return sentence\n","\n","print('Common functions for enhanced beam search done')\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Common functions for enhanced beam search done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mDXYlbrRN_P1","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"580ea06f-2ca5-48fa-a0f0-2409c683b7ab","executionInfo":{"status":"ok","timestamp":1573452357383,"user_tz":-480,"elapsed":551861,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["\"\"\"\n","def get_qam3_score(question, reply, score, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab):\n","\n","    question_inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding='post')\n","    question_inputs = tf.convert_to_tensor(question_inputs )\n","\n","    reply_outputs = [output_vocab.word2idx[i] for i in reply.split(' ')]\n","    reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding='post')\n","    reply_outputs = tf.convert_to_tensor(reply_outputs)\n","\n","    question_hidden = question_encoder.initialize_states()\n","    answer_hidden = answer_encoder.initialize_states()\n"," \n","    question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\n","    answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\n","    predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","    if label_vocab.idx2word[predicted_id] != '<pad>':\n","      matching_percentage = int(label_vocab.idx2word[predicted_id])\n","    else:\n","      matching_percentage = 0\n","\n","    qam3_score = (score + score*matching_percentage/100)/2\n","\n","    return qam3_score\n","\n","\n","def QAM3_beam_search_prediction_v2(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    enc_out, enc_hidden, _ = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    output_beam =[]\n","    done=False\n","\n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        \n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","            \n","            #predicted_sentence = remove_everything_after_end_token(predicted_sentence)\n","\n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    predicted_sentence += '<end>' \n","                    done = True\n","                    #break\n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","\n","                total_sent_prob += word_prob\n","                dec_input = tf.expand_dims([predicted_id], 0)\n","                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","            if done:\n","                break\n","        if done:\n","            break\n","\n","    for i in range(len(k_beam)):\n","        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\n","\n","    updated_output_beam=[]\n","    for replies in output_beam:\n","        score = replies[0]\n","        reply = preprocess_user_input_word(replies[1] ,output_vocab)\n","\n","        #print(question)\n","        question_inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding='post')\n","        question_inputs = tf.convert_to_tensor(question_inputs )\n","\n","        #print(reply)\n","        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(' ')]\n","        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding='post')\n","        reply_outputs = tf.convert_to_tensor(reply_outputs)\n","\n","        question_hidden = question_encoder.initialize_states()\n","        answer_hidden = answer_encoder.initialize_states()\n"," \n","        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\n","        answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\n","        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        if label_vocab.idx2word[predicted_id] != '<pad>':\n","          matching_percentage = int(label_vocab.idx2word[predicted_id])\n","        else:\n","          matching_percentage = 0\n","\n","        #print('Matching is {}'.format(matching_percentage))\n","        #try by using average\n","        new_score = (score + score*matching_percentage/100)/2\n","        #print('New score is {}'.format(new_score))\n","\n","        #try by using matching percentage only\n","        #new_score = score*matching_percentage\n","        \n","        #try by using 0.1 percentage\n","        #new_score = score + (0.1*score*matching_percentage/100)\n","\n","        updated_output_beam.append([new_score,reply])\n","        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\n","    \n","    predicted_sent = sorted_updated_output_beam [0][1]\n","    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\n","    \n","    return predicted_sent, avg_sent_prob\n","\n","def QAM3_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    enc_out, enc_hidden, _ = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    output_beam =[]\n","    done=False\n","\n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        \n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","            \n","            #predicted_sentence = remove_everything_after_end_token(predicted_sentence)\n","\n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    predicted_sentence += '<end>' \n","                    #done = True\n","                    #break\n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","\n","                total_sent_prob += word_prob\n","                dec_input = tf.expand_dims([predicted_id], 0)\n","                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","            #if done:\n","                #break\n","        #if done:\n","            #break\n","\n","    for i in range(len(k_beam)):\n","        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\n","\n","    updated_output_beam=[]\n","    for replies in output_beam:\n","        score = replies[0]\n","        reply = preprocess_user_input_word(replies[1] ,output_vocab)\n","\n","        #print(question)\n","        question_inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding='post')\n","        question_inputs = tf.convert_to_tensor(question_inputs )\n","\n","        #print(reply)\n","        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(' ')]\n","        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding='post')\n","        reply_outputs = tf.convert_to_tensor(reply_outputs)\n","\n","        question_hidden = question_encoder.initialize_states()\n","        answer_hidden = answer_encoder.initialize_states()\n"," \n","        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\n","        answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\n","        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        if label_vocab.idx2word[predicted_id] != '<pad>':\n","          matching_percentage = int(label_vocab.idx2word[predicted_id])\n","        else:\n","          matching_percentage = 0\n","\n","        #print('Matching is {}'.format(matching_percentage))\n","        #try by using average\n","        new_score = (score + score*matching_percentage/100)/2\n","        #print('New score is {}'.format(new_score))\n","\n","        #try by using matching percentage only\n","        #new_score = score*matching_percentage\n","        \n","        #try by using 0.1 percentage\n","        #new_score = score + (0.1*score*matching_percentage/100)\n","\n","        updated_output_beam.append([new_score,reply])\n","        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\n","    \n","    predicted_sent = sorted_updated_output_beam [0][1]\n","    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\n","    \n","    return predicted_sent, avg_sent_prob\n","\n","#Testing using Enhanced Beam Search - QAM3\n","batch_size=1   \n","label_vocab_path = param.label_vocab_path_qam3\n","label_vocab = get_vocab(label_vocab_path)\n","label_vocab_size=len(label_vocab.word2idx)\n","\n","optimizer = tf.train.AdamOptimizer() #tf v1\n","\n","#first restore the Baseline trained model\n","baseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","baseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded: {}\".format(checkpoint_filepath))\n","else:\n","  print(\"Checkpoint file is missing : {}\".format(checkpoint_filepath))\n","  \n","\n","#checkpoint_metafilename='checkpoint'\n","#checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","#checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \n","#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","#print('Checkpoint path {}'.format(checkpoint_filepath))\n","\n","\n","#then the QAM3 trained model for question-answer matching\n","qam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","qam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","qam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","qam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\n","\n","checkpoint_path_qam3 = param.checkpoint_path_qam3\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath_qam3 = os.path.join(checkpoint_path_qam3,checkpoint_metafilename)\n","if os.path.exists(checkpoint_filepath_qam3):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\n","  checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_qam3))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  #print(\"Checkpoint file is missing\")\n","  print(\"Checkpoint file is missing : {}\".format(checkpoint_filepath_qam3))\n","\n","#checkpoint_path_qam3 = param.checkpoint_path_qam3\n","#checkpoint_filepath_qam3 = os.path.join(checkpoint_path_qam3,checkpoint_metafilename)\n","#checkpoint_prefix_qam3 = os.path.join(checkpoint_path_qam3, \"ckpt\")\n","#checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\n","#checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_qam3))\n","#print('Checkpoint path {}'.format(checkpoint_filepath_qam3))\n","\n","start_row=0\n","test_size=1000\n","QA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\n","print('Questions path {}'.format(test_questions_path))\n","qa_pair=[]\n","\n","filename, file_extension = os.path.splitext(predictions_path)\n","beam_search_filename = filename + '_qam3_bs_Baseline.txt'\n","print('File path: {}'.format(beam_search_filename))\n","\n","#qa_pair_bs=[]\n","f = open(beam_search_filename, 'w')\n","\n","#beam search\n","for question, answer in QA_Pairs:\n","    if embedding_type == 'word':\n","        #takes care for char and mix\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        cleaned_input = preprocess_sentence_char(question)\n","    \n","    if len(cleaned_input.strip()):\n","        reply,  score = QAM3_beam_search_prediction(cleaned_input, rnntype, baseline_question_encoder, baseline_answer_decoder,  qam3_question_encoder, qam3_answer_encoder, qam3_label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,beam_size)    \n","        \n","        if embedding_type == 'char':\n","            reply = preprocess_output_char(reply)\n"," \n","        reply = re.sub('<start>', '', reply)\n","        #reply = re.sub('<end>', '', reply)\n","        \n","        idx = reply.find('<end>')\n","        if idx > 0:\n","            reply = reply[:idx]\n","        else:\n","            reply = re.sub('<end>', '', reply)\n","        \n","        reply = re.sub('<pad>', '', reply)\n","        #print(reply)\n","        #qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        line=question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() \n","        f.write(\"{}\\n\".format(line))\n","    else:\n","        reply = 'Zero words/characters in vocabulary'\n","        score=-1\n","        #qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        line=question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() \n","        f.write(\"{}\\n\".format(line))\n","        \n","#with open(beam_search_filename, 'w') as f:\n","#    for line in qa_pair_bs:\n","#        f.write(\"{}\\n\".format(line))\n","f.close()\n","\n","print('QAM3 Beam search done. File created at {}'.format(beam_search_filename))\n","\"\"\""],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef get_qam3_score(question, reply, score, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab):\\n\\n    question_inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n    question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding=\\'post\\')\\n    question_inputs = tf.convert_to_tensor(question_inputs )\\n\\n    reply_outputs = [output_vocab.word2idx[i] for i in reply.split(\\' \\')]\\n    reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding=\\'post\\')\\n    reply_outputs = tf.convert_to_tensor(reply_outputs)\\n\\n    question_hidden = question_encoder.initialize_states()\\n    answer_hidden = answer_encoder.initialize_states()\\n \\n    question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\\n    answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\\n    predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\\n\\n    predicted_id = tf.argmax(predictions[0]).numpy()\\n    if label_vocab.idx2word[predicted_id] != \\'<pad>\\':\\n      matching_percentage = int(label_vocab.idx2word[predicted_id])\\n    else:\\n      matching_percentage = 0\\n\\n    qam3_score = (score + score*matching_percentage/100)/2\\n\\n    return qam3_score\\n\\n\\ndef QAM3_beam_search_prediction_v2(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\\n    #attention_plot = np.zeros((max_length_targ, max_length_inp))\\n   \\n    inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding=\\'post\\')\\n    inputs = tf.convert_to_tensor(inputs)\\n    \\n    predicted_sentence = \\'\\'\\n   \\n    hidden = encoder.initialize_states()\\n    enc_out, enc_hidden, _ = encoder(inputs, hidden)\\n    \\n    dec_hidden = enc_hidden\\n    dec_input = tf.expand_dims([output_vocab.word2idx[\\'<start>\\']], 0)\\n    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\\n    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\\n    \\n    output_beam =[]\\n    done=False\\n\\n    for t in range(output_max_len):\\n        all_k_beams = []\\n        \\n        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\\n            \\n            #predicted_sentence = remove_everything_after_end_token(predicted_sentence)\\n\\n            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\\n            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\\n            \\n            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\\n                    \\n                if output_vocab.idx2word[predicted_id] == \\'<end>\\':\\n                    predicted_sentence += \\'<end>\\' \\n                    done = True\\n                    #break\\n                else:\\n                    predicted_sentence += output_vocab.idx2word[predicted_id] + \\' \\'\\n\\n                total_sent_prob += word_prob\\n                dec_input = tf.expand_dims([predicted_id], 0)\\n                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\\n                \\n            k_beam = sorted(all_k_beams,reverse=True)[:k]\\n            if done:\\n                break\\n        if done:\\n            break\\n\\n    for i in range(len(k_beam)):\\n        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\\n\\n    updated_output_beam=[]\\n    for replies in output_beam:\\n        score = replies[0]\\n        reply = preprocess_user_input_word(replies[1] ,output_vocab)\\n\\n        #print(question)\\n        question_inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding=\\'post\\')\\n        question_inputs = tf.convert_to_tensor(question_inputs )\\n\\n        #print(reply)\\n        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(\\' \\')]\\n        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding=\\'post\\')\\n        reply_outputs = tf.convert_to_tensor(reply_outputs)\\n\\n        question_hidden = question_encoder.initialize_states()\\n        answer_hidden = answer_encoder.initialize_states()\\n \\n        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\\n        answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\\n        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\\n\\n        predicted_id = tf.argmax(predictions[0]).numpy()\\n        if label_vocab.idx2word[predicted_id] != \\'<pad>\\':\\n          matching_percentage = int(label_vocab.idx2word[predicted_id])\\n        else:\\n          matching_percentage = 0\\n\\n        #print(\\'Matching is {}\\'.format(matching_percentage))\\n        #try by using average\\n        new_score = (score + score*matching_percentage/100)/2\\n        #print(\\'New score is {}\\'.format(new_score))\\n\\n        #try by using matching percentage only\\n        #new_score = score*matching_percentage\\n        \\n        #try by using 0.1 percentage\\n        #new_score = score + (0.1*score*matching_percentage/100)\\n\\n        updated_output_beam.append([new_score,reply])\\n        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\\n    \\n    predicted_sent = sorted_updated_output_beam [0][1]\\n    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\\n    \\n    return predicted_sent, avg_sent_prob\\n\\ndef QAM3_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\\n    #attention_plot = np.zeros((max_length_targ, max_length_inp))\\n   \\n    inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding=\\'post\\')\\n    inputs = tf.convert_to_tensor(inputs)\\n    \\n    predicted_sentence = \\'\\'\\n   \\n    hidden = encoder.initialize_states()\\n    enc_out, enc_hidden, _ = encoder(inputs, hidden)\\n    \\n    dec_hidden = enc_hidden\\n    dec_input = tf.expand_dims([output_vocab.word2idx[\\'<start>\\']], 0)\\n    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\\n    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\\n    \\n    output_beam =[]\\n    done=False\\n\\n    for t in range(output_max_len):\\n        all_k_beams = []\\n        \\n        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\\n            \\n            #predicted_sentence = remove_everything_after_end_token(predicted_sentence)\\n\\n            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\\n            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\\n            \\n            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\\n                    \\n                if output_vocab.idx2word[predicted_id] == \\'<end>\\':\\n                    predicted_sentence += \\'<end>\\' \\n                    #done = True\\n                    #break\\n                else:\\n                    predicted_sentence += output_vocab.idx2word[predicted_id] + \\' \\'\\n\\n                total_sent_prob += word_prob\\n                dec_input = tf.expand_dims([predicted_id], 0)\\n                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\\n                \\n            k_beam = sorted(all_k_beams,reverse=True)[:k]\\n            #if done:\\n                #break\\n        #if done:\\n            #break\\n\\n    for i in range(len(k_beam)):\\n        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\\n\\n    updated_output_beam=[]\\n    for replies in output_beam:\\n        score = replies[0]\\n        reply = preprocess_user_input_word(replies[1] ,output_vocab)\\n\\n        #print(question)\\n        question_inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding=\\'post\\')\\n        question_inputs = tf.convert_to_tensor(question_inputs )\\n\\n        #print(reply)\\n        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(\\' \\')]\\n        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding=\\'post\\')\\n        reply_outputs = tf.convert_to_tensor(reply_outputs)\\n\\n        question_hidden = question_encoder.initialize_states()\\n        answer_hidden = answer_encoder.initialize_states()\\n \\n        question_enc_output, question_enc_hidden, _ = question_encoder(question_inputs, question_hidden)\\n        answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\\n        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\\n\\n        predicted_id = tf.argmax(predictions[0]).numpy()\\n        if label_vocab.idx2word[predicted_id] != \\'<pad>\\':\\n          matching_percentage = int(label_vocab.idx2word[predicted_id])\\n        else:\\n          matching_percentage = 0\\n\\n        #print(\\'Matching is {}\\'.format(matching_percentage))\\n        #try by using average\\n        new_score = (score + score*matching_percentage/100)/2\\n        #print(\\'New score is {}\\'.format(new_score))\\n\\n        #try by using matching percentage only\\n        #new_score = score*matching_percentage\\n        \\n        #try by using 0.1 percentage\\n        #new_score = score + (0.1*score*matching_percentage/100)\\n\\n        updated_output_beam.append([new_score,reply])\\n        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\\n    \\n    predicted_sent = sorted_updated_output_beam [0][1]\\n    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\\n    \\n    return predicted_sent, avg_sent_prob\\n\\n#Testing using Enhanced Beam Search - QAM3\\nbatch_size=1   \\nlabel_vocab_path = param.label_vocab_path_qam3\\nlabel_vocab = get_vocab(label_vocab_path)\\nlabel_vocab_size=len(label_vocab.word2idx)\\n\\noptimizer = tf.train.AdamOptimizer() #tf v1\\n\\n#first restore the Baseline trained model\\nbaseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\nbaseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\nif os.path.exists(checkpoint_filepath):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \\n  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n  print(\"Existing checkpoint file loaded: {}\".format(checkpoint_filepath))\\nelse:\\n  print(\"Checkpoint file is missing : {}\".format(checkpoint_filepath))\\n  \\n\\n#checkpoint_metafilename=\\'checkpoint\\'\\n#checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\n#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\\n#checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \\n#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n#print(\\'Checkpoint path {}\\'.format(checkpoint_filepath))\\n\\n\\n#then the QAM3 trained model for question-answer matching\\nqam3_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\nqam3_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\nqam3_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\nqam3_label_decoder = SimilarityDecoder_RYANLOWE(label_vocab_size, units, dropout)\\n\\ncheckpoint_path_qam3 = param.checkpoint_path_qam3\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath_qam3 = os.path.join(checkpoint_path_qam3,checkpoint_metafilename)\\nif os.path.exists(checkpoint_filepath_qam3):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\\n  checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_qam3))\\n  print(\"Existing checkpoint file loaded\")\\nelse:\\n  #print(\"Checkpoint file is missing\")\\n  print(\"Checkpoint file is missing : {}\".format(checkpoint_filepath_qam3))\\n\\n#checkpoint_path_qam3 = param.checkpoint_path_qam3\\n#checkpoint_filepath_qam3 = os.path.join(checkpoint_path_qam3,checkpoint_metafilename)\\n#checkpoint_prefix_qam3 = os.path.join(checkpoint_path_qam3, \"ckpt\")\\n#checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\\n#checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_qam3))\\n#print(\\'Checkpoint path {}\\'.format(checkpoint_filepath_qam3))\\n\\nstart_row=0\\ntest_size=1000\\nQA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\\nprint(\\'Questions path {}\\'.format(test_questions_path))\\nqa_pair=[]\\n\\nfilename, file_extension = os.path.splitext(predictions_path)\\nbeam_search_filename = filename + \\'_qam3_bs_Baseline.txt\\'\\nprint(\\'File path: {}\\'.format(beam_search_filename))\\n\\n#qa_pair_bs=[]\\nf = open(beam_search_filename, \\'w\\')\\n\\n#beam search\\nfor question, answer in QA_Pairs:\\n    if embedding_type == \\'word\\':\\n        #takes care for char and mix\\n        cleaned_input = preprocess_user_input_word(question,input_vocab)\\n    else:\\n        cleaned_input = preprocess_sentence_char(question)\\n    \\n    if len(cleaned_input.strip()):\\n        reply,  score = QAM3_beam_search_prediction(cleaned_input, rnntype, baseline_question_encoder, baseline_answer_decoder,  qam3_question_encoder, qam3_answer_encoder, qam3_label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,beam_size)    \\n        \\n        if embedding_type == \\'char\\':\\n            reply = preprocess_output_char(reply)\\n \\n        reply = re.sub(\\'<start>\\', \\'\\', reply)\\n        #reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        idx = reply.find(\\'<end>\\')\\n        if idx > 0:\\n            reply = reply[:idx]\\n        else:\\n            reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        reply = re.sub(\\'<pad>\\', \\'\\', reply)\\n        #print(reply)\\n        #qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n        line=question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() \\n        f.write(\"{}\\n\".format(line))\\n    else:\\n        reply = \\'Zero words/characters in vocabulary\\'\\n        score=-1\\n        #qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n        line=question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() \\n        f.write(\"{}\\n\".format(line))\\n        \\n#with open(beam_search_filename, \\'w\\') as f:\\n#    for line in qa_pair_bs:\\n#        f.write(\"{}\\n\".format(line))\\nf.close()\\n\\nprint(\\'QAM3 Beam search done. File created at {}\\'.format(beam_search_filename))\\n'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"p__Ee9VXTEN2","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"d489b1c7-141e-4252-aa2f-9e70b7ac5d95","executionInfo":{"status":"ok","timestamp":1573452357385,"user_tz":-480,"elapsed":551859,"user":{"displayName":"Kulo Palasundram","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDJ6VUBvZ0PzoSRBEJTeqL_ycY9xOhnMj98VX5PZw=s64","userId":"18278279681651187172"}}},"source":["\"\"\"\n","def binary_classifier_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\n","    #attention_plot = np.zeros((max_length_targ, max_length_inp))\n","   \n","    inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    predicted_sentence = ''\n","   \n","    hidden = encoder.initialize_states()\n","    enc_out, enc_hidden , _ = encoder(inputs, hidden)\n","    \n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([output_vocab.word2idx['<start>']], 0)\n","    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\n","    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\n","    \n","    output_beam =[]\n","    \n","    for t in range(output_max_len):\n","        all_k_beams = []\n","        \n","        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\n","            \n","            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n","            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\n","            \n","            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\n","                    \n","                if output_vocab.idx2word[predicted_id] == '<end>':\n","                    predicted_sentence += '<end>' \n","                else:\n","                    predicted_sentence += output_vocab.idx2word[predicted_id] + ' '\n","\n","                total_sent_prob += word_prob\n","                dec_input = tf.expand_dims([predicted_id], 0)\n","                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\n","                \n","            k_beam = sorted(all_k_beams,reverse=True)[:k]\n","    \n","    for i in range(len(k_beam)):\n","        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\n","    \n","    updated_output_beam=[]\n","    for replies in output_beam:\n","        score = replies[0]\n","        reply = preprocess_user_input_word(replies[1] ,output_vocab)\n","        question_inputs = [input_vocab.word2idx[i] for i in question.split(' ')]\n","        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding='post')\n","        question_inputs = tf.convert_to_tensor(question_inputs )\n","\n","        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(' ')]\n","        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding='post')\n","        reply_outputs = tf.convert_to_tensor(reply_outputs)\n","\n","        question_hidden = question_encoder.initialize_states()\n","        answer_hidden = answer_encoder.initialize_states()\n","\n","        question_enc_output, question_enc_hidden , _ = question_encoder(question_inputs, question_hidden)\n","        answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\n","        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","        if label_vocab.idx2word[predicted_id] == 'positive':\n","          weight=1\n","        else:\n","          weight = 0\n","\n","        #new_score = (score + score*weight)/2\n","        new_score = score*weight\n","        updated_output_beam.append([new_score,reply])\n","        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\n","    \n","    predicted_sent = sorted_updated_output_beam [0][1]\n","    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\n","    \n","    return predicted_sent, avg_sent_prob\n","\n","#Testing using Enhanced Beam Search - QAM3\n","batch_size=1   \n","coreref_label_vocab_path = param.label_vocab_path_coreref\n","coreref_label_vocab = get_vocab(coreref_label_vocab_path)\n","coreref_label_vocab_size=len(coreref_label_vocab.word2idx)\n","optimizer = tf.train.AdamOptimizer() #tf v1\n","#optimizer = tf.optimizers.Adam()  #tf v2\n","\n","#first restore the Baseline trained model\n","baseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","baseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","if os.path.exists(checkpoint_filepath):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \n","  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Checkpoint file is missing\")\n","\n","#checkpoint_metafilename='checkpoint'\n","#checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\n","#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n","#checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \n","#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n","#print('Checkpoint path {}'.format(checkpoint_filepath))\n","\n","#then the Core Ref trained model for question-answer matching\n","coreref_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","coreref_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\n","coreref_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\n","coreref_label_decoder = SimilarityDecoder_RYANLOWE(coreref_label_vocab_size, units, dropout)\n","\n","checkpoint_path_coreref = param.checkpoint_path_coreref\n","checkpoint_metafilename='checkpoint'\n","checkpoint_filepath_coreref = os.path.join(checkpoint_path_coreref,checkpoint_metafilename)\n","if os.path.exists(checkpoint_filepath_coreref):\n","  print(\"Reloading existing checkpoint file\")\n","  checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, coreref_question_encoder=coreref_question_encoder, coreref_answer_decoder=coreref_answer_decoder, coreref_answer_encoder = coreref_answer_encoder, coreref_label_decoder=coreref_label_decoder)\n","  checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_coreref))\n","  print(\"Existing checkpoint file loaded\")\n","else:\n","  print(\"Checkpoint file is missing\")\n","\n","#checkpoint_path_qam3 = param.checkpoint_path_qam3\n","#checkpoint_filepath_qam3 = os.path.join(checkpoint_path_qam3,checkpoint_metafilename)\n","#checkpoint_prefix_qam3 = os.path.join(checkpoint_path_qam3, \"ckpt\")\n","#checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\n","#checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_qam3))\n","#print('Checkpoint path {}'.format(checkpoint_filepath_qam3))\n","\n","start_row=0\n","test_size=1000\n","QA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\n","print('Questions path {}'.format(test_questions_path))\n","qa_pair=[]\n","\n","filename, file_extension = os.path.splitext(predictions_path)\n","beam_search_filename = filename + '_coreref_bs_Baseline_usingweightonly.txt'\n","print('File path: {}'.format(beam_search_filename))\n","\n","qa_pair_bs=[]\n","\n","#beam search\n","for question, answer in QA_Pairs:\n","    if embedding_type == 'word':\n","        #takes care for char and mix\n","        cleaned_input = preprocess_user_input_word(question,input_vocab)\n","    else:\n","        cleaned_input = preprocess_sentence_char(question)\n","    \n","    if len(cleaned_input.strip()):\n","        reply,  score = binary_classifier_beam_search_prediction(cleaned_input, rnntype, baseline_question_encoder, baseline_answer_decoder,  coreref_question_encoder, coreref_answer_encoder, coreref_label_decoder, input_vocab, output_vocab, coreref_label_vocab, input_max_len, output_max_len,beam_size)    \n","        \n","        if embedding_type == 'char':\n","            reply = preprocess_output_char(reply)\n"," \n","        reply = re.sub('<start>', '', reply)\n","        #reply = re.sub('<end>', '', reply)\n","        \n","        idx = reply.find('<end>')\n","        if idx > 0:\n","            reply = reply[:idx]\n","        else:\n","            reply = re.sub('<end>', '', reply)\n","        \n","        reply = re.sub('<pad>', '', reply)\n","        #print(reply)\n","        qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","    else:\n","        reply = 'Zero words/characters in vocabulary'\n","        score=-1\n","        qa_pair_bs.append(question.strip()+'\\t'+  answer.strip() + '\\t' + reply.strip() )\n","        \n","with open(beam_search_filename, 'w') as f:\n","    for line in qa_pair_bs:\n","        f.write(\"{}\\n\".format(line))\n","\n","print('Binary Classifier beam search done. File created at {}'.format(beam_search_filename))\n","\"\"\""],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef binary_classifier_beam_search_prediction(question, rnntype, encoder, decoder, question_encoder, answer_encoder, label_decoder, input_vocab, output_vocab, label_vocab, input_max_len, output_max_len,k=1):\\n    #attention_plot = np.zeros((max_length_targ, max_length_inp))\\n   \\n    inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_max_len, padding=\\'post\\')\\n    inputs = tf.convert_to_tensor(inputs)\\n    \\n    predicted_sentence = \\'\\'\\n   \\n    hidden = encoder.initialize_states()\\n    enc_out, enc_hidden , _ = encoder(inputs, hidden)\\n    \\n    dec_hidden = enc_hidden\\n    dec_input = tf.expand_dims([output_vocab.word2idx[\\'<start>\\']], 0)\\n    total_sent_prob = 0 #average sentence probability, this is gettting the average (add all probs and divide by t)\\n    k_beam = [(total_sent_prob , predicted_sentence,dec_input,dec_hidden)]\\n    \\n    output_beam =[]\\n    \\n    for t in range(output_max_len):\\n        all_k_beams = []\\n        \\n        for total_sent_prob , predicted_sentence, dec_input, dec_hidden in k_beam:\\n            \\n            predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\\n            possible_k = get_k_top_predictions(predictions,total_sent_prob, predicted_sentence, dec_input,k)\\n            \\n            for predicted_id, word_prob,total_sent_prob,predicted_sentence, dec_input in possible_k:\\n                    \\n                if output_vocab.idx2word[predicted_id] == \\'<end>\\':\\n                    predicted_sentence += \\'<end>\\' \\n                else:\\n                    predicted_sentence += output_vocab.idx2word[predicted_id] + \\' \\'\\n\\n                total_sent_prob += word_prob\\n                dec_input = tf.expand_dims([predicted_id], 0)\\n                all_k_beams +=[(total_sent_prob, predicted_sentence,dec_input,dec_hidden)]\\n                \\n            k_beam = sorted(all_k_beams,reverse=True)[:k]\\n    \\n    for i in range(len(k_beam)):\\n        output_beam.append([k_beam[i][0], remove_everything_after_end_token(k_beam[i][1])])\\n    \\n    updated_output_beam=[]\\n    for replies in output_beam:\\n        score = replies[0]\\n        reply = preprocess_user_input_word(replies[1] ,output_vocab)\\n        question_inputs = [input_vocab.word2idx[i] for i in question.split(\\' \\')]\\n        question_inputs = tf.keras.preprocessing.sequence.pad_sequences([question_inputs], maxlen=input_max_len, padding=\\'post\\')\\n        question_inputs = tf.convert_to_tensor(question_inputs )\\n\\n        reply_outputs = [output_vocab.word2idx[i] for i in reply.split(\\' \\')]\\n        reply_outputs = tf.keras.preprocessing.sequence.pad_sequences([reply_outputs], maxlen=output_max_len, padding=\\'post\\')\\n        reply_outputs = tf.convert_to_tensor(reply_outputs)\\n\\n        question_hidden = question_encoder.initialize_states()\\n        answer_hidden = answer_encoder.initialize_states()\\n\\n        question_enc_output, question_enc_hidden , _ = question_encoder(question_inputs, question_hidden)\\n        answer_enc_output, answer_enc_hidden, _ = answer_encoder(reply_outputs, answer_hidden)\\n        predictions = label_decoder(question_enc_hidden , answer_enc_hidden )\\n        predicted_id = tf.argmax(predictions[0]).numpy()\\n        if label_vocab.idx2word[predicted_id] == \\'positive\\':\\n          weight=1\\n        else:\\n          weight = 0\\n\\n        #new_score = (score + score*weight)/2\\n        new_score = score*weight\\n        updated_output_beam.append([new_score,reply])\\n        sorted_updated_output_beam = sorted(updated_output_beam,reverse=True)\\n    \\n    predicted_sent = sorted_updated_output_beam [0][1]\\n    avg_sent_prob = (sorted_updated_output_beam [0][0])/(t+1)\\n    \\n    return predicted_sent, avg_sent_prob\\n\\n#Testing using Enhanced Beam Search - QAM3\\nbatch_size=1   \\ncoreref_label_vocab_path = param.label_vocab_path_coreref\\ncoreref_label_vocab = get_vocab(coreref_label_vocab_path)\\ncoreref_label_vocab_size=len(coreref_label_vocab.word2idx)\\noptimizer = tf.train.AdamOptimizer() #tf v1\\n#optimizer = tf.optimizers.Adam()  #tf v2\\n\\n#first restore the Baseline trained model\\nbaseline_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\nbaseline_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\nif os.path.exists(checkpoint_filepath):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \\n  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n  print(\"Existing checkpoint file loaded\")\\nelse:\\n  print(\"Checkpoint file is missing\")\\n\\n#checkpoint_metafilename=\\'checkpoint\\'\\n#checkpoint_filepath = os.path.join(checkpoint_path,checkpoint_metafilename)\\n#checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\\n#checkpoint = tf.train.Checkpoint(optimizer=optimizer, baseline_question_encoder=baseline_question_encoder, baseline_answer_decoder=baseline_answer_decoder)    \\n#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\\n#print(\\'Checkpoint path {}\\'.format(checkpoint_filepath))\\n\\n#then the Core Ref trained model for question-answer matching\\ncoreref_question_encoder = Encoder_BidirectionalGRU(input_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\ncoreref_answer_decoder = Decoder_GRU(output_vocab_size, embedding_dim, units*2, batch_size, dropout, recurrent_dropout)\\ncoreref_answer_encoder = Encoder_BidirectionalGRU(output_vocab_size, embedding_dim, units, batch_size, dropout, recurrent_dropout)\\ncoreref_label_decoder = SimilarityDecoder_RYANLOWE(coreref_label_vocab_size, units, dropout)\\n\\ncheckpoint_path_coreref = param.checkpoint_path_coreref\\ncheckpoint_metafilename=\\'checkpoint\\'\\ncheckpoint_filepath_coreref = os.path.join(checkpoint_path_coreref,checkpoint_metafilename)\\nif os.path.exists(checkpoint_filepath_coreref):\\n  print(\"Reloading existing checkpoint file\")\\n  checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, coreref_question_encoder=coreref_question_encoder, coreref_answer_decoder=coreref_answer_decoder, coreref_answer_encoder = coreref_answer_encoder, coreref_label_decoder=coreref_label_decoder)\\n  checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_coreref))\\n  print(\"Existing checkpoint file loaded\")\\nelse:\\n  print(\"Checkpoint file is missing\")\\n\\n#checkpoint_path_qam3 = param.checkpoint_path_qam3\\n#checkpoint_filepath_qam3 = os.path.join(checkpoint_path_qam3,checkpoint_metafilename)\\n#checkpoint_prefix_qam3 = os.path.join(checkpoint_path_qam3, \"ckpt\")\\n#checkpoint2 = tf.train.Checkpoint(optimizer=optimizer, qam3_question_encoder=qam3_question_encoder, qam3_answer_decoder=qam3_answer_decoder, qam3_answer_encoder = qam3_answer_encoder, qam3_label_decoder=qam3_label_decoder)\\n#checkpoint2.restore(tf.train.latest_checkpoint(checkpoint_path_qam3))\\n#print(\\'Checkpoint path {}\\'.format(checkpoint_filepath_qam3))\\n\\nstart_row=0\\ntest_size=1000\\nQA_Pairs = get_test_questions (test_questions_path,start_row, test_size)\\nprint(\\'Questions path {}\\'.format(test_questions_path))\\nqa_pair=[]\\n\\nfilename, file_extension = os.path.splitext(predictions_path)\\nbeam_search_filename = filename + \\'_coreref_bs_Baseline_usingweightonly.txt\\'\\nprint(\\'File path: {}\\'.format(beam_search_filename))\\n\\nqa_pair_bs=[]\\n\\n#beam search\\nfor question, answer in QA_Pairs:\\n    if embedding_type == \\'word\\':\\n        #takes care for char and mix\\n        cleaned_input = preprocess_user_input_word(question,input_vocab)\\n    else:\\n        cleaned_input = preprocess_sentence_char(question)\\n    \\n    if len(cleaned_input.strip()):\\n        reply,  score = binary_classifier_beam_search_prediction(cleaned_input, rnntype, baseline_question_encoder, baseline_answer_decoder,  coreref_question_encoder, coreref_answer_encoder, coreref_label_decoder, input_vocab, output_vocab, coreref_label_vocab, input_max_len, output_max_len,beam_size)    \\n        \\n        if embedding_type == \\'char\\':\\n            reply = preprocess_output_char(reply)\\n \\n        reply = re.sub(\\'<start>\\', \\'\\', reply)\\n        #reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        idx = reply.find(\\'<end>\\')\\n        if idx > 0:\\n            reply = reply[:idx]\\n        else:\\n            reply = re.sub(\\'<end>\\', \\'\\', reply)\\n        \\n        reply = re.sub(\\'<pad>\\', \\'\\', reply)\\n        #print(reply)\\n        qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n    else:\\n        reply = \\'Zero words/characters in vocabulary\\'\\n        score=-1\\n        qa_pair_bs.append(question.strip()+\\'\\t\\'+  answer.strip() + \\'\\t\\' + reply.strip() )\\n        \\nwith open(beam_search_filename, \\'w\\') as f:\\n    for line in qa_pair_bs:\\n        f.write(\"{}\\n\".format(line))\\n\\nprint(\\'Binary Classifier beam search done. File created at {}\\'.format(beam_search_filename))\\n'"]},"metadata":{"tags":[]},"execution_count":12}]}]}